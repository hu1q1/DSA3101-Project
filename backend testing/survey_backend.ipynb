{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LLM Capabilities for Convey - An Interactive Survey Interface\n",
    "\n",
    "In this notebook, we will explore the capabilities of Large Language Models (LLMs) for our project in building an interactive survey interface. We'll focus on the following tasks:\n",
    "\n",
    "## 1. RAG (Retrieval-Augmented Generation)\n",
    "- Implementing and fine-tuning RAG for tasks such as responding and asking follow-up questions to users in a personalised manner.\n",
    "- Exploring RAG's ability to provide relevant product-specific responses based on retrieval from a knowledge source.\n",
    "\n",
    "## 2. Prompt Engineering\n",
    "- Crafting effective prompts to guide the LLM's responses.\n",
    "- Experimenting with different prompt formats and strategies to optimise performance.\n",
    "\n",
    "## 3. Vector Store Manipulation\n",
    "- Manipulating vector stores to enhance the understanding and generation capabilities of the LLM.\n",
    "- Examining the impact of vector store modifications on the quality and relevance of generated responses.\n",
    "\n",
    "We'll use this notebook to test various features and functionalities provided by the LLM and assess its suitability for the Convey platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "1. Create and activate a virtual environment before running the command below to install the necessary Python packages.\n",
    "2. Create a hugging face api token and store it in the current working directory in a .env file as follows:\n",
    "\n",
    "    HUGGINGFACEHUB_API_TOKEN=\"hf_***************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from transformers import pipeline\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Hugging Face Hub API Token into OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from local .env file if available\n",
    "if os.path.isfile('.env'):\n",
    "    # Set path to api key\n",
    "    dotenv_path = Path('.env')\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "else:\n",
    "    load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Using Survey Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Survey Questions and Creating Document Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_questions = [\n",
    "    #{'id': 1, 'question': \"What is your name?\", \"check_user_response\": 0},   # This question is taken out and assumed as the first survey question\n",
    "    {'id': 2, 'question': \"What is your age group?\", \"check_user_response\": 0},\n",
    "    {'id': 3, 'question': \"what is your gender identity?\", \"check_user_response\": 0},\n",
    "]\n",
    "\n",
    "# Creating Document objects for survey questions\n",
    "demographic_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": -1,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in demographic_questions\n",
    "]\n",
    "\n",
    "stage_0_questions = [\n",
    "    {'id': 4, 'question': \"What is your hair length?\", \"check_user_response\": 0},\n",
    "    {'id': 5, 'question': \"What is your hair type?\", \"check_user_response\": 0},\n",
    "    {'id': 6, 'question': \"What are your hair concerns?\", \"check_user_response\": 0},\n",
    "    {'id': 7, 'question': \"What is your scalp type?\", \"check_user_response\": 0},\n",
    "    {'id': 8, 'question': \"What are your scalp concerns?\", \"check_user_response\": 0},\n",
    "    {'id': 9, 'question': \"What hair treatments have you done?\", \"check_user_response\": 0},\n",
    "]\n",
    "# Creating Document objects for survey questions\n",
    "stage_0_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": 0,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_0_questions\n",
    "]\n",
    "\n",
    "stage_1_questions = [\n",
    "    {'id': 10, 'question': \"How often do you wash your hair?\", \"check_user_response\": 0},\n",
    "    {'id': 11, 'question': \"What hair products do you use regularly?\", \"check_user_response\": 0},\n",
    "    {'id': 12, 'question': \"What hair styling products do you use regularly?\", \"check_user_response\": 0},\n",
    "    {'id': 13, 'question': \"How often do you switch hair product brands?\", \"check_user_response\": 0},\n",
    "    {'id': 14, 'question': \"How often do you visit hair salons or barber shops?\", \"check_user_response\": 0},\n",
    "    {'id': 15, 'question': \"What is your ideal hair goal?\", \"check_user_response\": 0},\n",
    "    {'id': 16, 'question': \"How important is hair health to you?\", \"check_user_response\": 0},\n",
    "]\n",
    "stage_1_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": 1,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_1_questions\n",
    "]\n",
    "\n",
    "stage_2_questions = [\n",
    "    {'id': 17, 'question': \"Which of the following Pantene product series (collections) are you aware of?\", \"check_user_response\": 0},\n",
    "    {'id': 18, 'question': \"From where did you know Pantene?\", \"check_user_response\": 0},\n",
    "    {'id': 19, 'question': \"What is your favorite Pantene product and what do you like about it?\", \"check_user_response\": 0},\n",
    "    {'id': 20, 'question': \"What is your least favorite Pantene product and what do you dislike about it?\", \"check_user_response\": 0},\n",
    "    {'id': 21, 'question': \"How would you rate the overall effectiveness Pantene products?\", \"check_user_response\": 0},\n",
    "    {'id': 22, 'question': \"Would you recommend your current hair products to others? Why?\", \"check_user_response\": 1},\n",
    "    {'id': 23, 'question': \"What hair product improvements would you like to see in the future?\", \"check_user_response\": 1},\n",
    "]\n",
    "stage_2_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": 2,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_2_questions\n",
    "]\n",
    "\n",
    "stage_3_questions = [\n",
    "    {'id': 24, 'question': \"When choosing hair products, how important are the following factors to you?\", \"check_user_response\": 0},\n",
    "    {'id': 25, 'question': \"What is your preferred price range for hair products?\", \"check_user_response\": 0},\n",
    "    {'id': 26, 'question': \"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\", \"check_user_response\": 1},\n",
    "]\n",
    "stage_3_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": 3,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_3_questions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt templates\n",
    "demographic_prompt_template = \"Hey there! Welcome to the survey! We're thrilled to have you on board. Let's kick things off by getting to know you a little better. Please take a moment to answer the following demographic questions:\\n{}\"\n",
    "stage_0_prompt_template = \"Great! Now, let's talk about your hair care routine. We're here to make sure our products match your needs perfectly. Share your thoughts with us:\\n{}\"\n",
    "stage_1_prompt_template = \"Awesome! We're diving deeper into your hair care habits and preferences. Your feedback is invaluable in helping us improve. Let's get started:\\n{}\"\n",
    "stage_2_prompt_template = \"You're doing great! Now, we're eager to hear what you think about Pantene products. Your insights will shape our future offerings. Share your thoughts with us:\\n{}\"\n",
    "stage_3_prompt_template = \"Almost there! We're curious about your shopping preferences and priorities. Let's wrap up with a few more questions:\\n{}\"\n",
    "\n",
    "# Few-shot examples\n",
    "# Demographic Questions\n",
    "demographic_few_shot_examples = [\n",
    "    (\"What is your age group?\", \"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"Above 65\"),\n",
    "    (\"What is your gender identity?\", \"Male\", \"Female\", \"Non-binary\", \"Prefer not to share\")\n",
    "]\n",
    "\n",
    "# Stage 0 Questions\n",
    "stage_0_few_shot_examples = [\n",
    "    (\"What is your hair length?\", \"Short\", \"Medium\", \"Long\", \"No hair\"),\n",
    "    (\"What is your hair type?\", \"Curly\", \"straight\", \"wavy\", \"dry\", \"normal\", \"oily\", \"thin\", \"thick\"),\n",
    "    (\"What are your hair concerns?\", \"Frizzy\", \"dry\", \"split ends\", \"hair loss\", \"breakage\", \"none\", \"others\"),\n",
    "    (\"What is your scalp type?\", \"Oily\", \"dry\", \"normal\"),\n",
    "    (\"What are your scalp concerns?\", \"Itchiness\", \"sensitive\", \"allergies\", \"dandruff\", \"dryness\", \"none\", \"others\"),\n",
    "    (\"What hair treatments have you done?\", \"Keratin treatments\", \"dyed\", \"permed\", \"bleached\", \"none\", \"others\")\n",
    "]\n",
    "\n",
    "# Stage 1 Questions\n",
    "stage_1_few_shot_examples = [\n",
    "    (\"How often do you wash your hair?\", \"Daily\", \"several times a day\", \"every other day\", \"others\"),\n",
    "    (\"What hair products do you use regularly?\", \"shampoo\", \"conditioner\", \"leave-in treatments\", \"hair masks\"),\n",
    "    (\"What hair styling products do you use regularly?\", \"gel\", \"hair dryer\", \"flat iron\", \"curler\", \"mousses\", \"serums\", \"others\"),\n",
    "    (\"How often do you switch hair product brands?\", \"every few months\", \"every year\", \"every few years\", \"I do not switch\"),\n",
    "    (\"How often do you visit hair salons or barber shops?\", \"every few weeks\", \"every few months\", \"once a year\", \"I do not visit\"),\n",
    "    (\"What is your ideal hair goal?\", \"Shiny\", \"healthy\", \"volume\", \"smoothness\", \"others\"),\n",
    "    (\"How important is hair health to you?\", \"Very important\", \"1\", \"5\", \"7\", \"10\")\n",
    "]\n",
    "\n",
    "# Stage 2 Questions\n",
    "stage_2_few_shot_examples = [\n",
    "    (\"Which of the following Pantene product series (collections) are you aware of?\", \"Pantene Pro-V\", \"Hair Care Shampoo and Conditioner\", \"I don't know any\"),\n",
    "    (\"From where did you know Pantene?\", \"TV commercials\", \"word of mouth\", \"retail shops\", \"social media\", \"others\"),\n",
    "    (\"What is your favorite Pantene product and what do you like about it?\", \"Pro-V shampoo, makes my hair soft\", \"conditioner, smells nice\"),\n",
    "    (\"What is your least favorite Pantene product and what do you dislike about it?\", \"Pantene conditioner, weighs down my hair\", \"conditioner, makes my hair fall\"),\n",
    "    (\"How would you rate the overall effectiveness Pantene products?\", \"Highly effective\", \"1\", \"5\", \"7\", \"10\"),\n",
    "    (\"Would you recommend your current hair products to others? Why?\", \"Yes, they make my hair feel great\", \"yes, they are affordabe\", \"no, there are better brands\", \"no, they made me drop more hair\"),\n",
    "    (\"What hair product improvements would you like to see in the future?\", \"More natural ingredients\", \"cheaper\", \"more benefits in a product\")\n",
    "]\n",
    "\n",
    "# Stage 3 Questions\n",
    "stage_3_few_shot_examples = [\n",
    "    (\"When choosing hair products, how important are the following factors to you?\", \"natural or synthetic ingredients\", \"fragrance\", \"specific certifications\", \"specific claims\", \"price\", \"celebrity endorsements or influencer recommendations\", \"specific hair concerns\", \"long-lasting effects\", \"multi-functional benefits\", \"eco-friendly or sustainable packaging\", \"hair stylists for salon professionals\", \"packaging\", \"advertising campaigns or promotions\"), \n",
    "    (\"What is your preferred price range for hair products?\", \"under $10\", \"$10-50\", \"$50-100\", \"above $100\"),\n",
    "    (\"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\", \"Online, Amazon\", \"online, shopee\", \"in store, NTUC\", \"in-store, salons\")\n",
    "]\n",
    "\n",
    "# Connect prompt templates for smooth conversation flow\n",
    "demographic_prompt = demographic_prompt_template.format(\"\\n\".join([q['question'] for q in demographic_questions]))\n",
    "stage_0_prompt = stage_0_prompt_template.format(\"\\n\".join([q['question'] for q in stage_0_questions]))\n",
    "stage_1_prompt = stage_1_prompt_template.format(\"\\n\".join([q['question'] for q in stage_1_questions]))\n",
    "stage_2_prompt = stage_2_prompt_template.format(\"\\n\".join([q['question'] for q in stage_2_questions]))\n",
    "stage_3_prompt = stage_3_prompt_template.format(\"\\n\".join([q['question'] for q in stage_3_questions]))\n",
    "\n",
    "# Define function to select prompt based on the stage of the survey\n",
    "def get_prompt(stage):\n",
    "    if stage == 0:\n",
    "        return stage_0_prompt\n",
    "    elif stage == 1:\n",
    "        return stage_1_prompt\n",
    "    elif stage == 2:\n",
    "        return stage_2_prompt\n",
    "    elif stage == 3:\n",
    "        return stage_3_prompt\n",
    "    else:\n",
    "        return \"Invalid stage number\"\n",
    "\n",
    "# Example usage:\n",
    "current_stage = 0\n",
    "current_prompt = get_prompt(current_stage)\n",
    "print(current_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Embedding Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an embedding model from Hugging Face\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='all-MiniLM-L6-v2', \n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employing FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorstore for the documents/survey questions\n",
    "demographic_db = FAISS.from_documents(\n",
    "    demographic_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "# Saving the vectorstore in local directory - persistence\n",
    "demographic_db.save_local(\"demographic_questions\")\n",
    "# Loading the vectorstore from local directory\n",
    "demographic_db = FAISS.load_local(\"demographic_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_0_db = FAISS.from_documents(\n",
    "    stage_0_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_0_db.save_local(\"stage_0_questions\")\n",
    "stage_0_db = FAISS.load_local(\"stage_0_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_1_db = FAISS.from_documents(\n",
    "    stage_1_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_1_db.save_local(\"stage_1_questions\")\n",
    "stage_1_db = FAISS.load_local(\"stage_1_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_2_db = FAISS.from_documents(\n",
    "    stage_2_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_2_db.save_local(\"stage_2_questions\")\n",
    "stage_2_db = FAISS.load_local(\"stage_2_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_3_db = FAISS.from_documents(\n",
    "    stage_3_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_3_db.save_local(\"stage_3_questions\")\n",
    "stage_3_db = FAISS.load_local(\"stage_3_questions\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"30 years old\"\n",
    "\n",
    "demographic_db.similarity_search_with_score(text, k=1, filter=dict(category='demographics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Open-source LLM from Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENDPOINT_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "ENDPOINT_URL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=250,\n",
    "    top_k=300,\n",
    "    temperature=1,\n",
    "    return_full_text=False,\n",
    "    streaming=True,\n",
    "    stop_sequences=['</s>'],\n",
    "    # callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever with Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(vectorstore: FAISS):\n",
    "    # Setting retriever to only retrieve the best follow-up question \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "    return retriever\n",
    "\n",
    "retriever = get_retriever(demographic_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating First Survey Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the first question\n",
    "def generate_first_question(question: str) -> str:\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        [INST] Welcome the survey respondent to my survey on hair routines and hair products in a friendly and cheerful language. Ask the first question given:\n",
    "\n",
    "        # Question:\n",
    "        {question}\n",
    "\n",
    "        [/INST]\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    output = chain.invoke({\"question\": question})\n",
    "    return output\n",
    "\n",
    "first_question = generate_first_question(\"What is your name?\")\n",
    "first_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chat Log Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging of chat\n",
    "def create_chat_log():\n",
    "    memory = ConversationBufferMemory(return_messages=False, memory_key='chat_history')\n",
    "    return memory\n",
    "\n",
    "def add_to_chat_log(chat_log, message_type: str, message: str):\n",
    "    if message_type == 'ai':\n",
    "        chat_log.chat_memory.add_ai_message(message)\n",
    "    else:\n",
    "        chat_log.chat_memory.add_user_message(message)\n",
    "\n",
    "def get_chat_history(chat_log):\n",
    "    chat_history = chat_log.load_memory_variables({})['chat_history']\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "chat_log = create_chat_log()\n",
    "add_to_chat_log(chat_log, message_type='ai', message=first_question)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.runnables import RunnableLambda - to be used for multiple arguments input\n",
    "\n",
    "def get_rag_chain(retriever):\n",
    "    # General prompt for all questions\n",
    "    #prompt_template = \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        [INST] As a friendly survey interface assistant, your task is to respond to the user's survey response in a personalized and friendly manner but do not ask any questions here.\n",
    "        Additionally, ask the follow-up question provided below.\n",
    "    \n",
    "        # Question:\n",
    "        {previous_question}\n",
    "        # User response:\n",
    "        {user_response}\n",
    "        # User sentiment:\n",
    "        {sentiment}\n",
    "        # Follow-up question:\n",
    "        {next_question}\n",
    "\n",
    "        Reply: [/INST]\"\"\"\n",
    "    )\n",
    "    \n",
    "    # prompt = PromptTemplate(\n",
    "    #     template=prompt_template, input_variables=['previous_question', 'user_response', 'next_question', 'sentiment']\n",
    "    # )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        # return \"\\n\\n\".join(doc.metadata['prompt'] + '\\n' + doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        # Retrieve next best question\n",
    "        RunnableParallel({\"docs\": itemgetter(\"user_response\") | retriever, \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        # Optional: Format question to ask user\n",
    "        | ({\"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"next_question\": lambda x: format_docs(x['docs']), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        # Optional: Prompt Engineering - Each question to have their own prompt template for LLM to ask the question\n",
    "        | ({\"docs\": lambda x: x['docs'], \"prompt\": prompt, \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"next_question\": itemgetter(\"next_question\"), \"previous_question\": itemgetter(\"previous_question\")}) \n",
    "        # Output results\n",
    "        | ({\"answer\": itemgetter(\"prompt\") | llm | StrOutputParser(), \"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "    )\n",
    "    return rag_chain \n",
    "\n",
    "\n",
    "rag_chain = get_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking RAG Chain with User Response to First Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_response = \"I am Xiao Ming.\"\n",
    "add_to_chat_log(chat_log, message_type='user', message=user_response)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of user response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error() \n",
    "\n",
    "def get_user_sentiment(user_response: str):\n",
    "    pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    user_sentiment = pipe(user_response)[0]['label']\n",
    "    return user_sentiment\n",
    "\n",
    "user_sentiment = get_user_sentiment(user_response)\n",
    "user_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_rag_chain(rag_chain, user_response: str, user_sentiment: str, previous_question: str):\n",
    "    output = {}\n",
    "    for chunk in rag_chain.stream(dict(user_response=user_response, sentiment=user_sentiment, previous_question=previous_question)):\n",
    "        for key in chunk:\n",
    "            if key not in output:\n",
    "                output[key] = chunk[key].strip() if key == 'answer' else chunk[key]\n",
    "            # if key == 'answer':\n",
    "                # new_token = chunk[key]\n",
    "                # yield new_token\n",
    "                # output[key] += new_token\n",
    "            else:\n",
    "                output[key] += chunk[key]\n",
    "            if key == 'answer':\n",
    "                print(chunk[key], end=\"\", flush=True)\n",
    "    return output\n",
    "    \n",
    "def get_llm_outputs(rag_chain, user_response: str, previous_question: str):\n",
    "    user_sentiment = get_user_sentiment(user_response)\n",
    "    output = invoke_rag_chain(rag_chain, user_response, user_sentiment, previous_question)\n",
    "    # LLM reply to output to frontend\n",
    "    llm_reply = output['answer']\n",
    "    # Get document of question asked by LLM \n",
    "    next_question_document = output['docs'][0]\n",
    "    # id of question asked to output to frontend \n",
    "    next_question_id = next_question_document.metadata['id']\n",
    "    return llm_reply, next_question_document, next_question_id\n",
    "\n",
    "\n",
    "llm_reply, next_question_document, next_question_id = get_llm_outputs(rag_chain, user_response, first_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Asked Question from Vector Store Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_question_from_db(vectorstore: FAISS, document_to_delete: Document):\n",
    "    count = 0\n",
    "    for key, item in vectorstore.docstore._dict.items():\n",
    "        count += 1\n",
    "        if item == document_to_delete:\n",
    "            break\n",
    "    if count >= 0:\n",
    "        vectorstore.delete([vectorstore.index_to_docstore_id[count-1]])\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "print(len(demographic_db.docstore._dict))\n",
    "demographic_db = remove_question_from_db(demographic_db, next_question_document)\n",
    "print(len(demographic_db.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Survey Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain to end the survey:\n",
    "def end_survey(user_response: str, question: str) -> str:\n",
    "    # print(\"It was interesting to get to know more about you! Thank you for participating in the survey!\")\n",
    "    # print(\"If you have any further questions or feedback, feel free to reach out to us.\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        [INST] Respond kindly to the user's input to the given question below. Avoid asking further questions at this stage. Finally, thank the survey participant for their participation warmly in a clear and exaggerated tone.\n",
    "\n",
    "        # User Response:\n",
    "        {response}\n",
    "        # Question:\n",
    "        {question}\n",
    "\n",
    "        [/INST]\"\"\"\n",
    "                                            \n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    output = chain.invoke({\"response\": user_response, \"question\": question})\n",
    "    return output\n",
    "\n",
    "# end_survey(\"Convenient to buy online\", \"Why buy online\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Ended Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What hair product improvements would you like to see in the future?\"\n",
    "question = \"Would you recommend your current hair products to others? Why?\"\n",
    "question = \"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess if Follow-Up Question is Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(user_response: str, question: str) -> dict:\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        [INST] Evaluate whether a follow-up question is necessary based on the user's response to the given question. Provide a \"Yes\" if a follow-up question is necessary or \"No\" otherwise, along with a confidence score between 0.0 and 1.0, and the reasoning. Your response should be in the form of a JSON object with the keys \"Assessment\" and \"Confidence\" and \"Reason\".\n",
    "\n",
    "        # User Response:\n",
    "        {response}\n",
    "\n",
    "        # Question:\n",
    "        {question}\n",
    "\n",
    "        [/INST]\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    output = chain.invoke({\"response\": user_response, \"question\": question})\n",
    "    return output\n",
    "\n",
    "# response = \"No, I don't like my products because they are consistently expensive, and despite the high cost, the quality is often subpar. Additionally, the products are notoriously difficult to find, which adds to the frustration of already dissatisfied customers. The combination of these factors makes it challenging to justify purchasing these products when there are more affordable and higher-quality alternatives available in the market. As a result, I am actively seeking alternative options that offer better value for money and a more satisfying shopping experience.\"\n",
    "# response = \"No\"\n",
    "response = \"Online\"\n",
    "\n",
    "assessment = evaluate_response(response, question)\n",
    "assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask a Follow-up Question Based on User Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFollowUp(user_response: str, question: str):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        [INST] You are a follow-up question generator. You are to provide a follow up question based on the given the survey user response to the question asked.\n",
    "        In clear and friendly tone and language, provide the follow-up question.\n",
    "        \n",
    "        # User Response:\n",
    "        {response}\n",
    "        \n",
    "        # Question:\n",
    "        {question}\n",
    "\n",
    "        Follow-up question: [/INST]\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    output = chain.invoke({\"response\": user_response, \"question\": question})\n",
    "    return output\n",
    "\n",
    "\n",
    "if assessment[\"Assessment\"] == \"Yes\":\n",
    "    follow_up_q = generateFollowUp(response, question)\n",
    "    print(follow_up_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Chain with Langchain Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"coherence\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"ahhahahah\"\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input='What is your gender identity?',\n",
    "    )\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_user_response(question,response):\n",
    "    eval_result = evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input=question,\n",
    "    )\n",
    "\n",
    "    return eval_result['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Simulation\n",
    "\n",
    "Make sure to run the above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Vector Store From Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_db = FAISS.load_local(\"demographic_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_0_db = FAISS.load_local(\"stage_0_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_1_db = FAISS.load_local(\"stage_1_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_2_db = FAISS.load_local(\"stage_2_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_3_db = FAISS.load_local(\"stage_3_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "if os.path.exists('history.json'):\n",
    "    os.remove('history.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question asked\n",
    "def get_question_asked(question_document):\n",
    "    # Retrieve original question based on question_id\n",
    "    return question_document.page_content\n",
    "\n",
    "# get_question_asked(next_question_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = None # Change this for testing different stages\n",
    "db = demographic_db\n",
    "retriever = get_retriever(demographic_db)\n",
    "question_asked = \"What is your name?\"\n",
    "user_response = \"\"\n",
    "next_question_document = None\n",
    "clarified = False\n",
    "\n",
    "first_question = generate_first_question(\"What is your name?\")\n",
    "print(f\"LLM: {first_question}\")\n",
    "\n",
    "# Create a json file to store the survey history \n",
    "history = pd.DataFrame({'id': [1], 'question': [\"What is your name?\"], 'llm_question': [first_question], 'user_response': [\"\"], 'stage': [-1]})\n",
    "history.to_json(\"history.json\", orient=\"records\")\n",
    "\n",
    "while True:\n",
    "    # User responded\n",
    "    if user_response:\n",
    "        \n",
    "        # Load in survey history\n",
    "        history = pd.read_json(\"history.json\")\n",
    "        # Add user response to history\n",
    "        history.loc[history.index[-1], \"user_response\"] = user_response\n",
    "\n",
    "        # Check user response for questions that are specified to check\n",
    "        if (next_question_document is not None) and (next_question_document.metadata['check'] == 1):\n",
    "            # Check if a follow up question is needed based on user response and the question asked\n",
    "            assessment = evaluate_response(user_response, question_asked)\n",
    "            needFollowUp = True if assessment[\"Assessment\"] == \"Yes\" else False\n",
    "            # If need follow up question, ask the question again\n",
    "            if needFollowUp:\n",
    "                # Allow only one follow-up per question i.e. repeat the question once\n",
    "                if clarified:\n",
    "                    clarified = False\n",
    "                    pass\n",
    "                else:\n",
    "                    clarified = True\n",
    "                    # TO DO: Improve the instruction or construct a LLM chain to ask the question again.\n",
    "                    follow_up_question = generateFollowUp(user_response, question_asked)\n",
    "                    print('\\n')\n",
    "                    print(f\"LLM: {follow_up_question}\")\n",
    "                    # Wait for user input\n",
    "                    user_response = input()\n",
    "                    print('\\n')\n",
    "                    print(\"User: \", end='')\n",
    "                    print(user_response)\n",
    "\n",
    "                    # Saving the question that the RAG chain has chosen to history\n",
    "                    new_row = pd.DataFrame({'id': [next_question_id], 'question': [question_asked], 'llm_question': [follow_up_question], 'user_response': [\"\"], 'stage': [next_question_document.metadata['stage']]})\n",
    "                    history = pd.concat([history, new_row], ignore_index=True)\n",
    "                    history.to_json(\"history.json\", orient=\"records\")\n",
    "                    continue\n",
    "\n",
    "        \n",
    "        # Survey flow\n",
    "        if len(db.docstore._dict) == 0 and stage is None:\n",
    "            stage = 0\n",
    "            db = stage_0_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 0:\n",
    "            stage = 1\n",
    "            db = stage_1_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 1:\n",
    "            stage = 2\n",
    "            db = stage_2_db\n",
    "        if len(db.docstore._dict) == 0 and stage == 2:\n",
    "            stage = 3\n",
    "            db = stage_3_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 3:\n",
    "            history.to_json(\"history.json\", orient=\"records\")\n",
    "            # To end the survey gracefully\n",
    "            end = end_survey(user_response, question_asked)\n",
    "            print('\\n')\n",
    "            print(\"LLM: {end}\")\n",
    "            break\n",
    "\n",
    "        ## Ask the next best question based on previous survey user response\n",
    "        # Create new retriever object with updated vectorstore\n",
    "        retriever = get_retriever(db)\n",
    "        # Create new RAG chain with updated retriever\n",
    "        qa_chain = get_rag_chain(retriever)\n",
    "        print('\\n')\n",
    "        print(\"LLM: \", end='')\n",
    "        # Get LLM reply, next question to ask and its question id\n",
    "        llm_reply, next_question_document, next_question_id = get_llm_outputs(qa_chain, user_response, question_asked)\n",
    "        # Get question asked\n",
    "        question_asked = get_question_asked(next_question_document)\n",
    "        # Updated vectorstore with asked question removed\n",
    "        db = remove_question_from_db(db, next_question_document)\n",
    "\n",
    "        # Saving the question that the RAG chain has chosen to history\n",
    "        new_row = pd.DataFrame({'id': [next_question_id], 'question': [next_question_document.page_content], 'llm_question': [llm_reply], 'user_response': [\"\"], \"stage\": [next_question_document.metadata['stage']]})\n",
    "        history = pd.concat([history, new_row], ignore_index=True)\n",
    "        history.to_json(\"history.json\", orient=\"records\")\n",
    "\n",
    "    # Wait for user input\n",
    "    user_response = input()\n",
    "    #user_response = f'The user\\'s response is:{user_response}'\n",
    "    print('\\n')\n",
    "    print(\"User: \", end='')\n",
    "    print(user_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My To Dos:\n",
    "# Add updated survey first question chain in utils\n",
    "# Add updated RAG prompt and LLM parameters in utils\n",
    "# Add evaluation chain of open-ended questions in utils\n",
    "# Add updated way in which history.json file is stored in utils\n",
    "# Add updated end survey chain in utils\n",
    "# Explore feature: adding conversation history into the RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mysql.connector\n",
    "\n",
    "load_dotenv()\n",
    "mysql_root_password = os.getenv(\"MYSQL_ROOT_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#run api test to get history.json\n",
    "with open('../../history.json', 'r') as file:\n",
    "    hist = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the n-th user_response in chat log\n",
    "def get_r(hist, id):\n",
    "    value = []\n",
    "    for chat in hist:\n",
    "        if chat['id'] == id:\n",
    "            value.append(chat['user_response'])\n",
    "    return ','.join(value)\n",
    "\n",
    "#accepts chat log and updates database\n",
    "def update_db(history):\n",
    "    #connect to database\n",
    "    db = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        port=3307,\n",
    "        user=\"root\",\n",
    "        password=mysql_root_password,\n",
    "    )\n",
    "    mycursor = db.cursor()\n",
    "\n",
    "    #add to database\n",
    "    mycursor.execute(\"USE testdatabase\")\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Stage_0(hair_length, hair_type, hair_concerns, scalp_type, scalp_concerns, hair_treatment) VALUES (%s,%s,%s,%s,%s,%s)\", (get_r(history,4), get_r(history,5), get_r(history,6), get_r(history,7), get_r(history,8), get_r(history,9))\n",
    "    )\n",
    "    stage0_id = mycursor.lastrowid\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Stage_1(wash_frequency, hair_products, styling_products, prod_switch_freq, salon_freq, hair_goal, hair_health_importance) VALUES (%s,%s,%s,%s,%s,%s,%s)\", (get_r(history,10), get_r(history,11), get_r(history,12), get_r(history,13), get_r(history,14), get_r(history,15),get_r(history,16))\n",
    "    )\n",
    "    stage1_id = mycursor.lastrowid\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Stage_2(pantene_prod, pantene_info, most_fav_product, least_fav_product, prod_effectiveness, prod_recommend, desired_ingredients) VALUES (%s,%s,%s,%s,%s,%s,%s)\", (get_r(history,17), get_r(history,18), get_r(history,19), get_r(history,20), get_r(history,21), get_r(history,22),get_r(history,23))\n",
    "    )\n",
    "    stage2_id = mycursor.lastrowid\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Stage_3(important_factors, preferred_price_range, purchase_method) VALUES (%s,%s,%s)\", (get_r(history,24), get_r(history,25), get_r(history,26))\n",
    "    )\n",
    "    stage3_id = mycursor.lastrowid\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Demographic(name, age, gender, stage0_id, stage1_id, stage2_id, stage3_id) VALUES (%s,%s,%s,%s,%s,%s,%s)\", (get_r(history,1), get_r(history,2), get_r(history,3),stage0_id,stage1_id,stage2_id,stage3_id)\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "\n",
    "    mycursor.close()\n",
    "    db.close()\n",
    "    print('db updated')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test one response per question\n",
    "test_json = [{\"id\":1,\"question\":\"What is your name?\",\"user_response\":\"the user's response is 'ti'\"},{\"id\":3,\"question\":\"what is your gender identity?\",\"user_response\":\"the user's response is 'male'\"},{\"id\":2,\"question\":\"What is your age group?\",\"user_response\":\"the user's response is '799'\"},{\"id\":4,\"question\":\"What is your hair length?\",\"user_response\":\"the user's response is 'lengthy'\"},{\"id\":5,\"question\":\"What is your hair type?\",\"user_response\":\"the user's response is 'brownian motion'\"},{\"id\":7,\"question\":\"What is your scalp type?\",\"user_response\":\"the user's response is 'the second law of thermodynamics'\"},{\"id\":6,\"question\":\"What are your hair concerns?\",\"user_response\":\"the user's response is 'e do be equal to mc squared'\"},{\"id\":8,\"question\":\"What are your scalp concerns?\",\"user_response\":\"the user's response is 'eahahahahhaha'\"},{\"id\":9,\"question\":\"What hair treatments have you done?\",\"user_response\":\"the user's response is 'no hair treatments'\"},{\"id\":11,\"question\":\"What hair products do you use regularly?\",\"user_response\":\"the user's response is \\\"none\\\"\"},{\"id\":15,\"question\":\"What is your ideal hair goal?\",\"user_response\":\"\"},{\"id\":16,\"user_response\":\"awwd\"},{\"id\":12,\"user_response\":\"a\"},{\"id\":13,\"user_response\":\"a\"},{\"id\":14,\"user_response\":\"a\"},{\"id\":10,\"user_response\":\"a\"},{\"id\":17,\"user_response\":\"a\"},{\"id\":18,\"user_response\":\"a\"},{\"id\":19,\"user_response\":\"a\"},{\"id\":20,\"user_response\":\"a\"},{\"id\":21,\"user_response\":\"a\"},{\"id\":22,\"user_response\":\"a\"},{\"id\":23,\"user_response\":\"a\"},{\"id\":24,\"user_response\":\"a\"},{\"id\":25,\"user_response\":\"a\"},{\"id\":26,\"user_response\":\"a\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test multiple response for same question\n",
    "test_double = [{\"id\":23,\"question\":\"What hair product improvements would you like to see in the future?\",\"llm_question\":\"I'm glad to hear that you're taking the time to share your thoughts with us! It seems like you're not aware of any Pantene product series at the moment. That's totally okay! We're always looking to improve and innovate, so I'm curious: what hair product improvements would you like to see in the future? Your input is truly valuable to us.\",\"user_response\":\"None\",\"stage\":2},\n",
    "    {\"id\":23,\"question\":\"What hair product improvements would you like to see in the future?\",\"llm_question\":\" I'm sorry for any confusion, but it seems like I didn't receive a response from you yet regarding the hair product improvements you'd like to see in the future. Your insights are valuable to us, so could you please share what changes or enhancements you'd like to see in hair products?\",\"user_response\":\"Ok priec\",\"stage\":2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to docker before running (docker start test-mysql)\n",
    "update_db(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
