{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LLM Capabilities for Convey - An Interactive Survey Interface\n",
    "\n",
    "In this notebook, we will explore the capabilities of Large Language Models (LLMs) for our project in building an interactive survey interface. We'll focus on the following tasks:\n",
    "\n",
    "## 1. RAG (Retrieval-Augmented Generation)\n",
    "- Implementing and fine-tuning RAG for tasks such as responding and asking follow-up questions to users in a personalised manner.\n",
    "- Exploring RAG's ability to provide relevant product-specific responses based on retrieval from a knowledge source.\n",
    "\n",
    "## 2. Prompt Engineering\n",
    "- Crafting effective prompts to guide the LLM's responses.\n",
    "- Experimenting with different prompt formats and strategies to optimise performance.\n",
    "\n",
    "## 3. Vector Store Manipulation\n",
    "- Manipulating vector stores to enhance the understanding and generation capabilities of the LLM.\n",
    "- Examining the impact of vector store modifications on the quality and relevance of generated responses.\n",
    "\n",
    "We'll use this notebook to test various features and functionalities provided by the LLM and assess its suitability for the Convey platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "1. Create and activate a virtual environment before running the command below to install the necessary Python packages.\n",
    "2. Create a hugging face api token and store it in the current working directory in a .env file as follows:\n",
    "\n",
    "    HUGGINGFACEHUB_API_TOKEN=\"hf_***************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import pipeline\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Hugging Face Hub API Token into OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from local .env file if available\n",
    "if os.path.isfile('.env'):\n",
    "    # Set path to api key\n",
    "    dotenv_path = Path('.env')\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "else:\n",
    "    load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Using Survey Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Survey Questions and Creating Document Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What is your gender identity?', metadata={'id': 0, 'category': 'demographics'}),\n",
       " Document(page_content='Where do you live?', metadata={'id': 1, 'category': 'demographics'}),\n",
       " Document(page_content='What is your approximate annual income?', metadata={'id': 2, 'category': 'demographics'}),\n",
       " Document(page_content='What is your employment status?', metadata={'id': 3, 'category': 'demographics'}),\n",
       " Document(page_content='What is the highest level of education you have completed?', metadata={'id': 4, 'category': 'demographics'}),\n",
       " Document(page_content='Do you have children under the age of 18?', metadata={'id': 5, 'category': 'demographics'}),\n",
       " Document(page_content='What is your marital status?', metadata={'id': 6, 'category': 'demographics'}),\n",
       " Document(page_content='What is your primary language?', metadata={'id': 7, 'category': 'demographics'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_questions = [\n",
    "    #\"What is your age?\",   # This question is taken out and assumed as the first survey question\n",
    "    \"What is your gender identity?\",\n",
    "    \"Where do you live?\",\n",
    "    \"What is your approximate annual income?\",\n",
    "    \"What is your employment status?\",\n",
    "    \"What is the highest level of education you have completed?\",\n",
    "    \"Do you have children under the age of 18?\",\n",
    "    \"What is your marital status?\",\n",
    "    \"What is your primary language?\",\n",
    "]\n",
    "\n",
    "# Creating Document objects for survey questions\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=question,\n",
    "        metadata={\n",
    "            \"id\":i,\n",
    "            \"category\":\"demographics\",\n",
    "            #\"prompt\":question_prompt\n",
    "        }\n",
    "    ) for i, question in enumerate(demographic_questions)\n",
    "]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Embedding Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an embedding model from Hugging Face\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='all-MiniLM-L6-v2', \n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employing FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorstore for the documents/survey questions\n",
    "db = FAISS.from_documents(\n",
    "    documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "# Saving the vectorstore in local directory - persistence\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "# Loading the vectorstore from local directory\n",
    "db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Do you have children under the age of 18?', metadata={'id': 5, 'category': 'demographics'}),\n",
       "  1.2642797)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"30 years old\"\n",
    "\n",
    "db.similarity_search_with_score(text, k=1, filter=dict(category='demographics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Open-source LLM from Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/elisalin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# ENDPOINT_URL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=128,\n",
    "    #top_k=50,\n",
    "    temperature=0.01,\n",
    "    #repetition_penalty=1.03,\n",
    "    return_full_text=False,\n",
    "    # callbacks=callbacks,\n",
    "    streaming=True,\n",
    "    stop_sequences=['</s>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever with Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(vectorstore: FAISS):\n",
    "    # Setting retriever to only retrieve the best follow-up question \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "    return retriever\n",
    "\n",
    "retriever = get_retriever(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating First Survey Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello there! I'm so excited to be part of your survey. I was wondering if you could tell me your age? Thank you!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask the first question\n",
    "first_question = llm.invoke(\"[INST]I am doing a survey. Greet me excitedly and ask me what is my age. Do not add anything.[/INST]\") #in a fairy tale setting\n",
    "\n",
    "first_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chat Log Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI:  Hello there! I'm so excited to be part of your survey. I was wondering if you could tell me your age? Thank you!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging of chat\n",
    "def create_chat_log():\n",
    "    memory = ConversationBufferMemory(return_messages=False, memory_key='chat_history')\n",
    "    return memory\n",
    "\n",
    "def add_to_chat_log(chat_log, message_type: str, message: str):\n",
    "    if message_type == 'ai':\n",
    "        chat_log.chat_memory.add_ai_message(message)\n",
    "    else:\n",
    "        chat_log.chat_memory.add_user_message(message)\n",
    "\n",
    "def get_chat_history(chat_log):\n",
    "    chat_history = chat_log.load_memory_variables({})['chat_history']\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "chat_log = create_chat_log()\n",
    "add_to_chat_log(chat_log, message_type='ai', message=first_question)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.runnables import RunnableLambda - to be used for multiple arguments input\n",
    "\n",
    "def get_rag_chain(retriever):\n",
    "    # General prompt for all questions\n",
    "    prompt_template = \"\"\"You are a friendly survey interface assistant.\n",
    "        You are given a survey user response, its sentiment and a follow-up question below.\n",
    "        Reply to the survey user kindly and ask the follow-up question.\n",
    "        Do not ask any other questions.\n",
    "\n",
    "        User response: {question}\n",
    "        User sentiment: {sentiment}\n",
    "        Follow-up question: {context}\n",
    "        \n",
    "        Reply:\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=['question', 'context', 'sentiment']\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        # return \"\\n\\n\".join(doc.metadata['prompt'] + '\\n' + doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        # Retrieve next best question\n",
    "        RunnableParallel({\"docs\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\"), \"sentiment\": itemgetter(\"sentiment\")})\n",
    "        # Optional: Format question to ask user\n",
    "        | ({\"docs\": lambda x: x['docs'], \"question\": itemgetter(\"question\"), \"sentiment\": itemgetter(\"sentiment\"), \"context\": lambda x: format_docs(x['docs'])})\n",
    "        # Optional: Prompt Engineering - Each question to have their own prompt template for LLM to ask the question\n",
    "        | ({\"docs\": lambda x: x['docs'], \"prompt\": prompt, \"question\": itemgetter(\"question\"), \"sentiment\": itemgetter(\"sentiment\"), \"context\": itemgetter(\"context\")}) \n",
    "        # Output results\n",
    "        | ({\"answer\": itemgetter(\"prompt\") | llm | StrOutputParser(), \"docs\": lambda x: x['docs'], \"question\": itemgetter(\"question\"), \"sentiment\": itemgetter(\"sentiment\") })\n",
    "    )\n",
    "    return rag_chain \n",
    "\n",
    "\n",
    "rag_chain = get_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking RAG Chain with User Response to First Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI:  Hello there! I'm so excited to be part of your survey. I was wondering if you could tell me your age? Thank you!\\nHuman: I am 24 years old.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_response = \"I am 24 years old.\"\n",
    "add_to_chat_log(chat_log, message_type='user', message=user_response)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of user response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.set_verbosity_error() \n",
    "\n",
    "def get_user_sentiment(user_response: str):\n",
    "    pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    user_sentiment = pipe(user_response)[0]['label']\n",
    "    return user_sentiment\n",
    "\n",
    "user_sentiment = get_user_sentiment(user_response)\n",
    "user_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Thank you for sharing your age with us! Do you have children under the age of 18?"
     ]
    }
   ],
   "source": [
    "def invoke_rag_chain(rag_chain, user_response: str, user_sentiment: str):\n",
    "    output = {}\n",
    "    for chunk in rag_chain.stream(dict(question=user_response, sentiment=user_sentiment)):\n",
    "        for key in chunk:\n",
    "            if key not in output:\n",
    "                output[key] = chunk[key].strip() if key == 'answer' else chunk[key]\n",
    "            # if key == 'answer':\n",
    "                # new_token = chunk[key]\n",
    "                # yield new_token\n",
    "                # output[key] += new_token\n",
    "            else:\n",
    "                output[key] += chunk[key]\n",
    "            if key == 'answer':\n",
    "                print(chunk[key], end=\"\", flush=True)\n",
    "    return output\n",
    "    \n",
    "def get_llm_outputs(rag_chain, user_response: str):\n",
    "    user_sentiment = get_user_sentiment(user_response)\n",
    "    output = invoke_rag_chain(rag_chain, user_response, user_sentiment)\n",
    "    # LLM reply to output to frontend\n",
    "    llm_reply = output['answer']\n",
    "    # Get document of question asked by LLM \n",
    "    next_question_document = output['docs'][0]\n",
    "    # id of question asked to output to frontend \n",
    "    next_question_id = next_question_document.metadata['id']\n",
    "    return llm_reply, next_question_document, next_question_id\n",
    "\n",
    "\n",
    "llm_reply, next_question_document, next_question_id = get_llm_outputs(rag_chain, user_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Asked Question from Vector Store Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def remove_question_from_db(vectorstore: FAISS, document_to_delete: Document):\n",
    "    count = 0\n",
    "    for key, item in vectorstore.docstore._dict.items():\n",
    "        count += 1\n",
    "        if item == document_to_delete:\n",
    "            break\n",
    "    if count >= 0:\n",
    "        vectorstore.delete([vectorstore.index_to_docstore_id[count-1]])\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "print(len(db.docstore._dict))\n",
    "db = remove_question_from_db(db, next_question_document)\n",
    "print(len(db.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Simulation\n",
    "\n",
    "Make sure to run the above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Vector Store From Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "len(db.docstore._dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for participating in the survey!\n",
      "If you have any further questions or feedback, feel free to reach out to us.\n"
     ]
    }
   ],
   "source": [
    "chat_log = create_chat_log()\n",
    "retriever = get_retriever(db)\n",
    "\n",
    "while True:\n",
    "    if len(db.docstore._dict) == 0:\n",
    "        # To end the survey gracefully\n",
    "        end_survey()\n",
    "        break  # Exit the loop when the survey ends\n",
    "\n",
    "    chat_history = get_chat_history(chat_log)\n",
    "    # Begin survey if chat history is empty\n",
    "    if chat_history == '':\n",
    "        first_question = llm.invoke(\"[INST]I am starting to answer a survey. Greet me and ask me what is my age.[/INST]\")\n",
    "        print(f\"LLM: {first_question}\")\n",
    "    # Ask the next best question based on previous survey user response\n",
    "    else:\n",
    "        # Create new retriever object with updated vectorstore\n",
    "        retriever = get_retriever(db)\n",
    "        # Create new RAG chain with updated retriever\n",
    "        qa_chain = get_rag_chain(retriever)\n",
    "        print('\\n')\n",
    "        print(\"LLM: \", end='')\n",
    "        llm_reply, next_question_document, next_question_id = get_llm_outputs(qa_chain, user_response)\n",
    "        add_to_chat_log(chat_log, message_type='ai', message=llm_reply)\n",
    "        # Updated vectorstore with asked question removed\n",
    "        db = remove_question_from_db(db, next_question_document)\n",
    "    \n",
    "    # Wait for user input\n",
    "    user_response = input()\n",
    "    print('\\n')\n",
    "    print(\"User: \", end='')\n",
    "    print(user_response)\n",
    "    add_to_chat_log(chat_log, message_type='user', message=user_response)\n",
    "\n",
    "    # LLM chain to end the survey:\n",
    "    def end_survey():\n",
    "        print(\"It was interesting to get to know more about you!Thank you for participating in the survey!\")\n",
    "        print(\"If you have any further questions or feedback, feel free to reach out to us.\")\n",
    "\n",
    "    # Get back original question:\n",
    "    def get_original_question(question_id):\n",
    "        # Retrieve original question based on question_id\n",
    "        original_question = db.docstore._dict[question_id].page_content\n",
    "        return original_question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
