{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LLM Capabilities for Convey - An Interactive Survey Interface\n",
    "\n",
    "In this notebook, we will explore the capabilities of Large Language Models (LLMs) for our project in building an interactive survey interface. We'll focus on the following tasks:\n",
    "\n",
    "## 1. RAG (Retrieval-Augmented Generation)\n",
    "- Implementing and fine-tuning RAG for tasks such as responding and asking follow-up questions to users in a personalised manner.\n",
    "- Exploring RAG's ability to provide relevant product-specific responses based on retrieval from a knowledge source.\n",
    "\n",
    "## 2. Prompt Engineering\n",
    "- Crafting effective prompts to guide the LLM's responses.\n",
    "- Experimenting with different prompt formats and strategies to optimise performance.\n",
    "\n",
    "## 3. Vector Store Manipulation\n",
    "- Manipulating vector stores to enhance the understanding and generation capabilities of the LLM.\n",
    "- Examining the impact of vector store modifications on the quality and relevance of generated responses.\n",
    "\n",
    "We'll use this notebook to test various features and functionalities provided by the LLM and assess its suitability for the Convey platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "1. Create and activate a virtual environment before running the command below to install the necessary Python packages.\n",
    "2. Create a hugging face api token and store it in the current working directory in a .env file as follows:\n",
    "\n",
    "    HUGGINGFACEHUB_API_TOKEN=\"hf_***************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from transformers import pipeline\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Hugging Face Hub API Token into OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from local .env file if available\n",
    "if os.path.isfile('.env'):\n",
    "    # Set path to api key\n",
    "    dotenv_path = Path('.env')\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "else:\n",
    "    load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Using Survey Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Survey Questions and Creating Document Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_questions = [\n",
    "    #{'id': 1, 'question': \"What is your name?\", \"check_user_response\": 0},   # This question is taken out and assumed as the first survey question\n",
    "    {'id': 2, 'question': \"What is your age group?\", \"check_user_response\": 0},\n",
    "    {'id': 3, 'question': \"what is your gender identity?\", \"check_user_response\": 0},\n",
    "]\n",
    "\n",
    "# Creating Document objects for survey questions\n",
    "demographic_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": -1,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in demographic_questions\n",
    "]\n",
    "\n",
    "stage_0_questions = [\n",
    "    {'id': 4, 'question': \"What is your hair length?\", \"check_user_response\": 0},\n",
    "    {'id': 5, 'question': \"What is your hair type?\", \"check_user_response\": 0},\n",
    "    {'id': 6, 'question': \"What are your hair concerns?\", \"check_user_response\": 0},\n",
    "    {'id': 7, 'question': \"What is your scalp type?\", \"check_user_response\": 0},\n",
    "    {'id': 8, 'question': \"What are your scalp concerns?\", \"check_user_response\": 0},\n",
    "    {'id': 9, 'question': \"What hair treatments have you done?\", \"check_user_response\": 0},\n",
    "]\n",
    "# Creating Document objects for survey questions\n",
    "stage_0_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": 0,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_0_questions\n",
    "]\n",
    "\n",
    "stage_1_questions = [\n",
    "    {'id': 10, 'question': \"How often do you wash your hair?\", \"check_user_response\": 0},\n",
    "    {'id': 11, 'question': \"What hair products do you use regularly?\", \"check_user_response\": 0},\n",
    "    {'id': 12, 'question': \"What hair styling products do you use regularly?\", \"check_user_response\": 0},\n",
    "    {'id': 13, 'question': \"How often do you switch hair product brands?\", \"check_user_response\": 0},\n",
    "    {'id': 14, 'question': \"How often do you visit hair salons or barber shops?\", \"check_user_response\": 0},\n",
    "    {'id': 15, 'question': \"What is your ideal hair goal?\", \"check_user_response\": 0},\n",
    "    {'id': 16, 'question': \"How important is hair health to you?\", \"check_user_response\": 0},\n",
    "]\n",
    "stage_1_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": 1,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_1_questions\n",
    "]\n",
    "\n",
    "stage_2_questions = [\n",
    "    {'id': 17, 'question': \"Which of the following Pantene product series (collections) are you aware of?\", \"check_user_response\": 0},\n",
    "    {'id': 18, 'question': \"From where did you know Pantene?\", \"check_user_response\": 0},\n",
    "    {'id': 19, 'question': \"What is your favorite Pantene product and what do you like about it?\", \"check_user_response\": 0},\n",
    "    {'id': 20, 'question': \"What is your least favorite Pantene product and what do you dislike about it?\", \"check_user_response\": 0},\n",
    "    {'id': 21, 'question': \"How would you rate the overall effectiveness Pantene products?\", \"check_user_response\": 0},\n",
    "    {'id': 22, 'question': \"Would you recommend your current hair products to others? Why?\", \"check_user_response\": 1},\n",
    "    {'id': 23, 'question': \"What hair product improvements would you like to see in the future?\", \"check_user_response\": 1},\n",
    "]\n",
    "stage_2_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": 2,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_2_questions\n",
    "]\n",
    "\n",
    "stage_3_questions = [\n",
    "    {'id': 24, 'question': \"When choosing hair products, how important are the following factors to you?\", \"check_user_response\": 0},\n",
    "    {'id': 25, 'question': \"What is your preferred price range for hair products?\", \"check_user_response\": 0},\n",
    "    {'id': 26, 'question': \"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\", \"check_user_response\": 1},\n",
    "]\n",
    "stage_3_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": 3,\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_3_questions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! Now, let's talk about your hair care routine. We're here to make sure our products match your needs perfectly. Share your thoughts with us:\n",
      "What is your hair length?\n",
      "What is your hair type?\n",
      "What are your hair concerns?\n",
      "What is your scalp type?\n",
      "What are your scalp concerns?\n",
      "What hair treatments have you done?\n"
     ]
    }
   ],
   "source": [
    "# Define prompt templates\n",
    "demographic_prompt_template = \"Hey there! Welcome to the survey! We're thrilled to have you on board. Let's kick things off by getting to know you a little better. Please take a moment to answer the following demographic questions:\\n{}\"\n",
    "stage_0_prompt_template = \"Great! Now, let's talk about your hair care routine. We're here to make sure our products match your needs perfectly. Feel free to your thoughts with us:\\n{}\"\n",
    "stage_1_prompt_template = \"Awesome! We're diving deeper into your hair care habits and preferences. Your feedback is invaluable in helping us improve. Let's get started:\\n{}\"\n",
    "stage_2_prompt_template = \"You're doing great! Now, we're eager to hear what you think about Pantene products. Your insights will shape our future offerings. Share your thoughts with us:\\n{}\"\n",
    "stage_3_prompt_template = \"Almost there! We're curious about your shopping preferences and priorities. Let's wrap up with a few more questions:\\n{}\"\n",
    "\n",
    "# Few-shot examples\n",
    "# Demographic Questions\n",
    "demographic_few_shot_examples = [\n",
    "    (\"What is your age group?\", \"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"Above 65\"),\n",
    "    (\"What is your gender identity?\", \"Male\", \"Female\", \"Non-binary\", \"Prefer not to share\")\n",
    "]\n",
    "\n",
    "# Stage 0 Questions\n",
    "stage_0_few_shot_examples = [\n",
    "    (\"What is your hair length?\", \"Short\", \"Medium\", \"Long\", \"No hair\"),\n",
    "    (\"What is your hair type?\", \"Curly\", \"straight\", \"wavy\", \"dry\", \"normal\", \"oily\", \"thin\", \"thick\"),\n",
    "    (\"What are your hair concerns?\", \"Frizzy\", \"dry\", \"split ends\", \"hair loss\", \"breakage\", \"none\", \"others\"),\n",
    "    (\"What is your scalp type?\", \"Oily\", \"dry\", \"normal\"),\n",
    "    (\"What are your scalp concerns?\", \"Itchiness\", \"sensitive\", \"allergies\", \"dandruff\", \"dryness\", \"none\", \"others\"),\n",
    "    (\"What hair treatments have you done?\", \"Keratin treatments\", \"dyed\", \"permed\", \"bleached\", \"none\", \"others\")\n",
    "]\n",
    "\n",
    "# Stage 1 Questions\n",
    "stage_1_few_shot_examples = [\n",
    "    (\"How often do you wash your hair?\", \"Daily\", \"several times a day\", \"every other day\", \"others\"),\n",
    "    (\"What hair products do you use regularly?\", \"shampoo\", \"conditioner\", \"leave-in treatments\", \"hair masks\"),\n",
    "    (\"What hair styling products do you use regularly?\", \"gel\", \"hair dryer\", \"flat iron\", \"curler\", \"mousses\", \"serums\", \"others\"),\n",
    "    (\"How often do you switch hair product brands?\", \"every few months\", \"every year\", \"every few years\", \"I do not switch\"),\n",
    "    (\"How often do you visit hair salons or barber shops?\", \"every few weeks\", \"every few months\", \"once a year\", \"I do not visit\"),\n",
    "    (\"What is your ideal hair goal?\", \"Shiny\", \"healthy\", \"volume\", \"smoothness\", \"others\"),\n",
    "    (\"How important is hair health to you?\", \"Very important\", \"1\", \"5\", \"7\", \"10\")\n",
    "]\n",
    "\n",
    "# Stage 2 Questions\n",
    "stage_2_few_shot_examples = [\n",
    "    (\"Which of the following Pantene product series (collections) are you aware of?\", \"Pantene Pro-V\", \"Hair Care Shampoo and Conditioner\", \"I don't know any\"),\n",
    "    (\"From where did you know Pantene?\", \"TV commercials\", \"word of mouth\", \"retail shops\", \"social media\", \"others\"),\n",
    "    (\"What is your favorite Pantene product and what do you like about it?\", \"Pro-V shampoo, makes my hair soft\", \"conditioner, smells nice\"),\n",
    "    (\"What is your least favorite Pantene product and what do you dislike about it?\", \"Pantene conditioner, weighs down my hair\", \"conditioner, makes my hair fall\"),\n",
    "    (\"How would you rate the overall effectiveness Pantene products?\", \"Highly effective\", \"1\", \"5\", \"7\", \"10\"),\n",
    "    (\"Would you recommend your current hair products to others? Why?\", \"Yes, they make my hair feel great\", \"yes, they are affordabe\", \"no, there are better brands\", \"no, they made me drop more hair\"),\n",
    "    (\"What hair product improvements would you like to see in the future?\", \"More natural ingredients\", \"cheaper\", \"more benefits in a product\")\n",
    "]\n",
    "\n",
    "# Stage 3 Questions\n",
    "stage_3_few_shot_examples = [\n",
    "    (\"When choosing hair products, how important are the following factors to you?\", \"natural or synthetic ingredients\", \"fragrance\", \"specific certifications\", \"specific claims\", \"price\", \"celebrity endorsements or influencer recommendations\", \"specific hair concerns\", \"long-lasting effects\", \"multi-functional benefits\", \"eco-friendly or sustainable packaging\", \"hair stylists for salon professionals\", \"packaging\", \"advertising campaigns or promotions\"), \n",
    "    (\"What is your preferred price range for hair products?\", \"under $10\", \"$10-50\", \"$50-100\", \"above $100\"),\n",
    "    (\"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\", \"Online, Amazon\", \"online, shopee\", \"in store, NTUC\", \"in-store, salons\")\n",
    "]\n",
    "\n",
    "# Connect prompt templates for smooth conversation flow\n",
    "demographic_prompt = demographic_prompt_template.format(\"\\n\".join([q['question'] for q in demographic_questions]))\n",
    "stage_0_prompt = stage_0_prompt_template.format(\"\\n\".join([q['question'] for q in stage_0_questions]))\n",
    "stage_1_prompt = stage_1_prompt_template.format(\"\\n\".join([q['question'] for q in stage_1_questions]))\n",
    "stage_2_prompt = stage_2_prompt_template.format(\"\\n\".join([q['question'] for q in stage_2_questions]))\n",
    "stage_3_prompt = stage_3_prompt_template.format(\"\\n\".join([q['question'] for q in stage_3_questions]))\n",
    "\n",
    "# Define function to select prompt based on the stage of the survey\n",
    "def get_prompt(stage):\n",
    "    if stage == 0:\n",
    "        return stage_0_prompt\n",
    "    elif stage == 1:\n",
    "        return stage_1_prompt\n",
    "    elif stage == 2:\n",
    "        return stage_2_prompt\n",
    "    elif stage == 3:\n",
    "        return stage_3_prompt\n",
    "    else:\n",
    "        return \"Invalid stage number\"\n",
    "\n",
    "# Example usage:\n",
    "current_stage = 0\n",
    "current_prompt = get_prompt(current_stage)\n",
    "print(current_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Embedding Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an embedding model from Hugging Face\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='all-MiniLM-L6-v2', \n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employing FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorstore for the documents/survey questions\n",
    "demographic_db = FAISS.from_documents(\n",
    "    demographic_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "# Saving the vectorstore in local directory - persistence\n",
    "demographic_db.save_local(\"demographic_questions\")\n",
    "# Loading the vectorstore from local directory\n",
    "demographic_db = FAISS.load_local(\"demographic_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_0_db = FAISS.from_documents(\n",
    "    stage_0_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_0_db.save_local(\"stage_0_questions\")\n",
    "stage_0_db = FAISS.load_local(\"stage_0_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_1_db = FAISS.from_documents(\n",
    "    stage_1_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_1_db.save_local(\"stage_1_questions\")\n",
    "stage_1_db = FAISS.load_local(\"stage_1_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_2_db = FAISS.from_documents(\n",
    "    stage_2_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_2_db.save_local(\"stage_2_questions\")\n",
    "stage_2_db = FAISS.load_local(\"stage_2_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_3_db = FAISS.from_documents(\n",
    "    stage_3_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_3_db.save_local(\"stage_3_questions\")\n",
    "stage_3_db = FAISS.load_local(\"stage_3_questions\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"30 years old\"\n",
    "\n",
    "demographic_db.similarity_search_with_score(text, k=1, filter=dict(category='demographics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Open-source LLM from Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/elisalin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# ENDPOINT_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "ENDPOINT_URL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=250,\n",
    "    top_k=300,\n",
    "    temperature=1,\n",
    "    return_full_text=False,\n",
    "    streaming=True,\n",
    "    stop_sequences=['</s>'],\n",
    "    # callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever with Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(vectorstore: FAISS):\n",
    "    # Setting retriever to only retrieve the best follow-up question \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "    return retriever\n",
    "\n",
    "retriever = get_retriever(demographic_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating First Survey Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello and a warm welcome to you! I'm thrilled you've decided to take part in our hair routines and hair products survey. Your insights will be incredibly valuable to us. \\n\\nTo kick things off, could you please tell us your name? We're excited to get to know you better!\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask the first question\n",
    "def generate_first_question(question: str) -> str:\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        [INST] Welcome the survey respondent to my survey on hair routines and hair products in a friendly and cheerful language. Ask the first question given:\n",
    "\n",
    "        # Question:\n",
    "        {question}\n",
    "\n",
    "        [/INST]\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    output = chain.invoke({\"question\": question})\n",
    "    return output\n",
    "\n",
    "first_question = generate_first_question(\"What is your name?\")\n",
    "first_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chat Log Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI:  Hello and a warm welcome to you! I'm thrilled you've decided to take part in our hair routines and hair products survey. Your insights will be incredibly valuable to us. \\n\\nTo kick things off, could you please tell us your name? We're excited to get to know you better!\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging of chat\n",
    "def create_chat_log():\n",
    "    memory = ConversationBufferMemory(return_messages=False, memory_key='chat_history')\n",
    "    return memory\n",
    "\n",
    "def add_to_chat_log(chat_log, message_type: str, message: str):\n",
    "    if message_type == 'ai':\n",
    "        chat_log.chat_memory.add_ai_message(message)\n",
    "    else:\n",
    "        chat_log.chat_memory.add_user_message(message)\n",
    "\n",
    "def get_chat_history(chat_log):\n",
    "    chat_history = chat_log.load_memory_variables({})['chat_history']\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "chat_log = create_chat_log()\n",
    "add_to_chat_log(chat_log, message_type='ai', message=first_question)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.runnables import RunnableLambda - to be used for multiple arguments input\n",
    "\n",
    "def get_rag_chain(retriever):\n",
    "    # General prompt for all questions\n",
    "    #prompt_template = \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        [INST] As a friendly survey interface assistant, your task is to respond to the user's survey response in a personalized and friendly manner but do not ask any questions here.\n",
    "        Additionally, ask the follow-up question provided below.\n",
    "    \n",
    "        # Question:\n",
    "        {previous_question}\n",
    "        # User response:\n",
    "        {user_response}\n",
    "        # User sentiment:\n",
    "        {sentiment}\n",
    "        # Follow-up question:\n",
    "        {next_question}\n",
    "\n",
    "        Reply: [/INST]\"\"\"\n",
    "    )\n",
    "    \n",
    "    # prompt = PromptTemplate(\n",
    "    #     template=prompt_template, input_variables=['previous_question', 'user_response', 'next_question', 'sentiment']\n",
    "    # )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        # return \"\\n\\n\".join(doc.metadata['prompt'] + '\\n' + doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        # Retrieve next best question\n",
    "        RunnableParallel({\"docs\": itemgetter(\"user_response\") | retriever, \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        # Optional: Format question to ask user\n",
    "        | ({\"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"next_question\": lambda x: format_docs(x['docs']), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        # Optional: Prompt Engineering - Each question to have their own prompt template for LLM to ask the question\n",
    "        | ({\"docs\": lambda x: x['docs'], \"prompt\": prompt, \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"next_question\": itemgetter(\"next_question\"), \"previous_question\": itemgetter(\"previous_question\")}) \n",
    "        # Output results\n",
    "        | ({\"answer\": itemgetter(\"prompt\") | llm | StrOutputParser(), \"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        | ({\"answer\": lambda x: x['answer'].replace(\"Hello!\", \"\").replace(\"Hey there!\", \"\").replace(\"Hello there!\", \"Oh, I see!\") if any(greet in x['answer'] for greet in [\"Hello!\", \"Hey there!\", \"Hello there!\"]) else x['answer'], \"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "    )\n",
    "    return rag_chain \n",
    "\n",
    "\n",
    "rag_chain = get_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking RAG Chain with User Response to First Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI:  Hello and a warm welcome to you! I'm thrilled you've decided to take part in our hair routines and hair products survey. Your insights will be incredibly valuable to us. \\n\\nTo kick things off, could you please tell us your name? We're excited to get to know you better!\\nHuman: I am Xiao Ming.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_response = \"I am Xiao Ming.\"\n",
    "add_to_chat_log(chat_log, message_type='user', message=user_response)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of user response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.set_verbosity_error() \n",
    "\n",
    "def get_user_sentiment(user_response: str):\n",
    "    pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    user_sentiment = pipe(user_response)[0]['label']\n",
    "    return user_sentiment\n",
    "\n",
    "user_sentiment = get_user_sentiment(user_response)\n",
    "user_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Xiao Ming, it's nice to meet you! Thank you for sharing your name with us. \n",
      "\n",
      "Now, let's move on to the next question. Could you please tell us what your age group is? We're keen to learn more about our survey participants."
     ]
    }
   ],
   "source": [
    "def invoke_rag_chain(rag_chain, user_response: str, user_sentiment: str, previous_question: str):\n",
    "    output = {}\n",
    "    for chunk in rag_chain.stream(dict(user_response=user_response, sentiment=user_sentiment, previous_question=previous_question)):\n",
    "        for key in chunk:\n",
    "            if key not in output:\n",
    "                output[key] = chunk[key].strip() if key == 'answer' else chunk[key]\n",
    "            # if key == 'answer':\n",
    "                # new_token = chunk[key]\n",
    "                # yield new_token\n",
    "                # output[key] += new_token\n",
    "            else:\n",
    "                output[key] += chunk[key]\n",
    "            if key == 'answer':\n",
    "                print(chunk[key], end=\"\", flush=True)\n",
    "    return output\n",
    "    \n",
    "def get_llm_outputs(rag_chain, user_response: str, previous_question: str):\n",
    "    user_sentiment = get_user_sentiment(user_response)\n",
    "    output = invoke_rag_chain(rag_chain, user_response, user_sentiment, previous_question)\n",
    "    # LLM reply to output to frontend\n",
    "    llm_reply = output['answer']\n",
    "    # Get document of question asked by LLM \n",
    "    next_question_document = output['docs'][0]\n",
    "    # id of question asked to output to frontend \n",
    "    next_question_id = next_question_document.metadata['id']\n",
    "    return llm_reply, next_question_document, next_question_id\n",
    "\n",
    "\n",
    "llm_reply, next_question_document, next_question_id = get_llm_outputs(rag_chain, user_response, first_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Asked Question from Vector Store Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def remove_question_from_db(vectorstore: FAISS, document_to_delete: Document):\n",
    "    count = 0\n",
    "    for key, item in vectorstore.docstore._dict.items():\n",
    "        count += 1\n",
    "        if item == document_to_delete:\n",
    "            break\n",
    "    if count >= 0:\n",
    "        vectorstore.delete([vectorstore.index_to_docstore_id[count-1]])\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "print(len(demographic_db.docstore._dict))\n",
    "demographic_db = remove_question_from_db(demographic_db, next_question_document)\n",
    "print(len(demographic_db.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Survey Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain to end the survey:\n",
    "def end_survey(user_response: str, question: str) -> str:\n",
    "    # print(\"It was interesting to get to know more about you! Thank you for participating in the survey!\")\n",
    "    # print(\"If you have any further questions or feedback, feel free to reach out to us.\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        [INST] Respond kindly to the user's input to the given question below. Avoid asking further questions at this stage. Finally, thank the survey participant for their participation warmly in a clear and exaggerated tone.\n",
    "\n",
    "        # User Response:\n",
    "        {response}\n",
    "        # Question:\n",
    "        {question}\n",
    "\n",
    "        [/INST]\"\"\"\n",
    "                                            \n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    output = chain.invoke({\"response\": user_response, \"question\": question})\n",
    "    return output\n",
    "\n",
    "# end_survey(\"Convenient to buy online\", \"Why buy online\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Ended Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What hair product improvements would you like to see in the future?\"\n",
    "question = \"Would you recommend your current hair products to others? Why?\"\n",
    "question = \"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess if Follow-Up Question is Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Assessment': 'Yes',\n",
       " 'Confidence': 0.9,\n",
       " 'Reason': \"The user's response 'Online' only answers the first part of the question. The second part of the question, 'If in-store, which stores?' remains unanswered, thus a follow-up question is necessary to gather a complete response.\"}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_response(user_response: str, question: str) -> dict:\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        [INST] Evaluate whether a follow-up question is necessary based on the user's response to the given question. Provide a \"Yes\" if a follow-up question is necessary or \"No\" otherwise, along with a confidence score between 0.0 and 1.0, and the reasoning. Your response should be in the form of a JSON object with the keys \"Assessment\" and \"Confidence\" and \"Reason\".\n",
    "\n",
    "        # User Response:\n",
    "        {response}\n",
    "\n",
    "        # Question:\n",
    "        {question}\n",
    "\n",
    "        [/INST]\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    output = chain.invoke({\"response\": user_response, \"question\": question})\n",
    "    return output\n",
    "\n",
    "# response = \"No, I don't like my products because they are consistently expensive, and despite the high cost, the quality is often subpar. Additionally, the products are notoriously difficult to find, which adds to the frustration of already dissatisfied customers. The combination of these factors makes it challenging to justify purchasing these products when there are more affordable and higher-quality alternatives available in the market. As a result, I am actively seeking alternative options that offer better value for money and a more satisfying shopping experience.\"\n",
    "# response = \"No\"\n",
    "response = \"Online\"\n",
    "\n",
    "assessment = evaluate_response(response, question)\n",
    "assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask a Follow-up Question Based on User Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " That's interesting! So, you prefer buying hair products online. Could you tell me what particularly attracts you to online shopping for hair products? Is it the convenience, the variety, or something else? I'd love to hear your thoughts!\n"
     ]
    }
   ],
   "source": [
    "def generateFollowUp(user_response: str, question: str):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        [INST] You are a follow-up question generator. You are to provide a follow up question based on the given the survey user response to the question asked.\n",
    "        In clear and friendly tone and language, provide the follow-up question.\n",
    "        \n",
    "        # User Response:\n",
    "        {response}\n",
    "        \n",
    "        # Question:\n",
    "        {question}\n",
    "\n",
    "        Follow-up question: [/INST]\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    output = chain.invoke({\"response\": user_response, \"question\": question})\n",
    "    return output\n",
    "\n",
    "\n",
    "if assessment[\"Assessment\"] == \"Yes\":\n",
    "    follow_up_q = generateFollowUp(response, question)\n",
    "    print(follow_up_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Chain with Langchain Presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"coherence\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Step 1: Determine if the submission is coherent\\nThe submission \"ahhahahah\" does not seem to be coherent. It does not contain any information about the user\\'s gender identity.\\n\\nStep 2: Determine if the submission is well-structured\\nThe submission \"ahhahahah\" is not well-structured. It does not have a clear organization or format.\\n\\nStep 3: Determine if the submission is organized\\nThe submission \"ahhahahah\" does not appear to be organized. It does not contain any logical flow of ideas or concepts.\\n\\nAnswer:\\nN',\n",
       " 'value': 'N',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = \"ahhahahah\"\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input='What is your gender identity?',\n",
    "    )\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_user_response(question,response):\n",
    "    eval_result = evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input=question,\n",
    "    )\n",
    "\n",
    "    return eval_result['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Simulation\n",
    "\n",
    "Make sure to run the above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Vector Store From Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_db = FAISS.load_local(\"demographic_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_0_db = FAISS.load_local(\"stage_0_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_1_db = FAISS.load_local(\"stage_1_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_2_db = FAISS.load_local(\"stage_2_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_3_db = FAISS.load_local(\"stage_3_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "if os.path.exists('history.json'):\n",
    "    os.remove('history.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get question asked\n",
    "def get_question_asked(question_document):\n",
    "    # Retrieve original question based on question_id\n",
    "    return question_document.page_content\n",
    "\n",
    "# get_question_asked(next_question_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:  Hello and a warm welcome to you! I'm thrilled you've decided to take part in our hair routines and hair products survey. Your insights will be incredibly valuable to us. \n",
      "\n",
      "To kick things off, could you please tell us your name? We're excited to get to know you better!\n",
      "\n",
      "\n",
      "User: shalala\n",
      "\n",
      "\n",
      "LLM:  Hello Shalala! It's nice to meet you. To continue getting to know you better, could you share what your gender identity is?\n",
      "\n",
      "User: m\n",
      "\n",
      "\n",
      "LLM:  Thanks for sharing that with me! I'm here to make this conversation as comfortable as possible. You identified as male, which is great. Now, could you please tell me what your age group is?\n",
      "\n",
      "User: 24\n",
      "\n",
      "\n",
      "LLM:  That's great! You're in the prime of your life at 24. Now, let's talk about something different. What about your hair? Could you tell me what your hair length is?\n",
      "\n",
      "User: short\n",
      "\n",
      "\n",
      "LLM:  That's great, you have short hair! It's so versatile and easy to manage. I bet it looks fantastic on you. Now, I'm curious to know, what is your hair type? Is it straight, wavy, curly, or something else?\n",
      "\n",
      "User: straight\n",
      "\n",
      "\n",
      "LLM:  That's great, you have straight hair! It can be so sleek and shiny. Now, let's talk about your scalp. What is your scalp type?\n",
      "\n",
      "User: normal\n",
      "\n",
      "\n",
      "LLM:  That's great to know that you have a normal scalp type! It's always wonderful to have a balanced scalp. Now, I'm curious, what are your hair concerns? Are you looking for ways to maintain its health, or perhaps you're dealing with a specific issue like dryness, oiliness, or something else? I'm here to help, so feel free to share!\n",
      "\n",
      "User: dont have\n",
      "\n",
      "\n",
      "LLM:   It's great to hear that you don't have any hair concerns at the moment. It's important to keep your hair healthy and happy! Just out of curiosity, what hair treatments have you done in the past?\n",
      "\n",
      "User: none\n",
      "\n",
      "\n",
      "LLM:  Hello there! It's great to hear that you haven't had any hair treatments done. That means your hair is all natural, which is wonderful! Since we're on the topic of hair care, I'd love to know more about your scalp concerns, if any. What are they, if you feel comfortable sharing?\n",
      "\n",
      "User: dh\n",
      "\n",
      "\n",
      "LLM:   It's great to have you here. You mentioned 'dh' as your scalp concern. If there's more you'd like to share or if you need any specific information about this concern, feel free to let me know. \n",
      "\n",
      "Moving on to your hair goals, what is your ideal hair goal? Is there a particular hairstyle, hair health, or growth rate you're aiming for? I'm here to help with any information you might need.\n",
      "\n",
      "User: nice hair\n",
      "\n",
      "\n",
      "LLM:  I'm glad to hear you're thinking about your hair in such a positive way! When it comes to your locks, how important is hair health to you on a scale of 1-10?\n",
      "\n",
      "User: 7\n",
      "\n",
      "\n",
      "LLM:  That's great to hear that hair health is important to you with a score of 7! It's wonderful to prioritize self-care. Now, moving on to our next question, how often do you switch hair product brands? We're always curious to learn more about our user's routines.\n",
      "\n",
      "User: i dont\n",
      "\n",
      "\n",
      "LLM:  Hey there! It's great that you've found a hair care routine that works for you. Could you share with me what hair styling products you use regularly? I'm always on the lookout for new recommendations! ðŸ˜Š\n",
      "\n",
      "User: hair gel\n",
      "\n",
      "\n",
      "LLM:  That's great, hair gel is a versatile product! I'm just curious, what other hair products do you use regularly to keep your hair looking fabulous?\n",
      "\n",
      "User: nothing\n",
      "\n",
      "\n",
      "LLM:  No worries at all, not everyone uses hair products! As for washing, how often do you wash your hair?\n",
      "\n",
      "User: daily\n",
      "\n",
      "\n",
      "LLM:  That's great to know that you wash your hair daily! It's important to maintain good hygiene. Now, moving on to another topic, how often do you visit hair salons or barber shops?\n",
      "\n",
      "User: monthly\n",
      "\n",
      "\n",
      "LLM:  That's great! Visiting hair salons or barber shops monthly is a good way to maintain your hairstyle. \n",
      "\n",
      "As for your hair care routine at home, which of the following Pantene product series (collections) are you aware of? The options are:\n",
      "1. Pantene Pro-V\n",
      "2. Pantene Daily Moisture Renewal\n",
      "3. Pantene Nutrient Blends\n",
      "4. Pantene Intense Repair\n",
      "Let me know which ones you've heard of or used before!\n",
      "\n",
      "User: no\n",
      "\n",
      "\n",
      "LLM:  I'm glad you're taking the time to share your thoughts with me! It's totally fine if you're not aware of the Pantene product series. To help us better understand your preferences, would you mind sharing whether you would recommend your current hair products to others? We're curious to know what you appreciate about them!\n",
      "\n",
      "User: ya its cheap\n",
      "\n",
      "\n",
      "LLM:  That's great to hear that it's affordable! I'm curious, despite the low cost, do you find that your current hair products still perform well and meet your expectations in terms of quality?\n",
      "\n",
      "\n",
      "User: ya they smell nice too\n",
      "\n",
      "\n",
      "LLM:  That's great to hear that you like the smell of your hair products! It's always a bonus when our beauty products not only work well but also leave a pleasant scent. \n",
      "\n",
      "As for the future, it's always exciting to imagine what improvements could be made. Do you have any thoughts on what you'd like to see in future hair products? It could be anything from a new scent, improved effectiveness, or even a different formulation. We're all ears!\n",
      "\n",
      "User: nope no thought\n",
      "\n",
      "\n",
      "LLM:  That's completely okay if you don't have any specific improvements in mind right now. Just to clarify, you mentioned \"nope no thought\" in relation to hair product improvements. Does that mean you're currently satisfied with the hair products you're using and see no need for change, or is it that you haven't really given it much thought yet? I'm here to understand your perspective better.\n",
      "\n",
      "\n",
      "User: ys i guess im satisfied\n",
      "\n",
      "\n",
      "LLM:  That's great to hear that you're generally satisfied with the current hair products! It's always good to know when people are happy with what's available.\n",
      "\n",
      "When it comes to Pantene products, could you share which one is your least favorite and why you don't enjoy it as much? Your feedback is truly valuable in helping us understand what works and what might need a bit more attention in the future.\n",
      "\n",
      "User: i dont use pantene sorry\n",
      "\n",
      "\n",
      "LLM:  No worries at all, not everyone uses Pantene! It's great that you're sharing your thoughts. I'm just curious, from where did you know about Pantene? Is it from advertisements, friends, or other sources? I'm always interested in understanding how people learn about various brands.\n",
      "\n",
      "User: suoermarkets\n",
      "\n",
      "\n",
      "LLM:   It's great to hear that you've discovered Pantene at supermarkets. That's a common place for many people to find and try new products. \n",
      "\n",
      "Now, I'm curious about your personal favorite from Pantene. Could you share what your favorite Pantene product is and what you like most about it? I'm always excited to learn about others' experiences with various hair care products!\n",
      "\n",
      "User: dont use pantene so dont have\n",
      "\n",
      "\n",
      "LLM:  No worries at all, not everyone uses Pantene! As for the overall effectiveness of Pantene products, people seem to have mixed opinions. Some find them very effective, while others prefer different brands. If you have any experience with other hair care products, we'd love to hear your thoughts!\n",
      "\n",
      "User: idk cuz i dont use pantene\n",
      "\n",
      "\n",
      "LLM:  That's totally okay if you don't use Pantene! I was just curious about your experience. Since you're here, could you share what your preferred price range is for hair products? This will help us understand your preferences better.\n",
      "\n",
      "User: 10-30\n",
      "\n",
      "\n",
      "LLM:  That's great to know that your preferred price range for hair products is between 10-30! It's important to find products that fit your budget.\n",
      "\n",
      "As for your follow-up question, when choosing hair products, the factors that I'd like to know about are:\n",
      "1. Ingredients: Are there any specific ingredients you look for or avoid in hair products?\n",
      "2. Brand reputation: How much does the brand's reputation matter to you?\n",
      "3. Product effectiveness: How important is it that the product works well for your hair type?\n",
      "4. Price: Is the price a significant factor in your decision-making process?\n",
      "5. Eco-friendliness: Do you consider how eco-friendly or sustainable the product is?\n",
      "\n",
      "Please feel free to share your thoughts on any or all of these factors. I'm here to learn more about your preferences to provide you with a better and more personalized experience! ðŸ˜Š\n",
      "\n",
      "User: price is\n",
      "\n",
      "\n",
      "LLM:  Thanks for sharing your thoughts on the importance of price when choosing hair products! It's good to know where you stand.\n",
      "\n",
      "Now, let's talk about your shopping preferences. Do you prefer to purchase hair products online or in-store? If in-store, which stores do you usually visit? We're always looking for ways to improve your shopping experience, so your input is valuable. Looking forward to hearing from you!\n",
      "\n",
      "User: online\n",
      "\n",
      "\n",
      "LLM:  That's interesting! So, you prefer purchasing hair products online. Could you tell me more about what you like most about shopping for hair products online?\n",
      "\n",
      "\n",
      "User: heyyy\n",
      "\n",
      "\n",
      "LLM:  Hello there! It's great to hear from you. When it comes to buying hair products, I personally enjoy the in-store experience. It gives me a chance to physically see and test the products before purchasing. Some of my favorite stores for hair products include Ulta, Sephora, and local beauty supply stores.\n",
      "\n",
      "Thank you so much for participating in this survey! Your input is truly valuable to us. We appreciate your time and effort in helping us improve our services. Keep sharing your thoughts and opinions - they really do make a difference! Have a fantastic day!\n"
     ]
    }
   ],
   "source": [
    "stage = None # Change this for testing different stages\n",
    "db = demographic_db\n",
    "retriever = get_retriever(demographic_db)\n",
    "question_asked = \"What is your name?\"\n",
    "user_response = \"\"\n",
    "next_question_document = None\n",
    "clarified = False\n",
    "\n",
    "first_question = generate_first_question(\"What is your name?\")\n",
    "print(f\"LLM: {first_question}\")\n",
    "\n",
    "# Create a json file to store the survey history \n",
    "history = pd.DataFrame({'id': [1], 'question': [\"What is your name?\"], 'llm_question': [first_question], 'user_response': [\"\"], 'stage': [-1]})\n",
    "history.to_json(\"history.json\", orient=\"records\")\n",
    "\n",
    "while True:\n",
    "    # User responded\n",
    "    if user_response:\n",
    "        \n",
    "        # Load in survey history\n",
    "        history = pd.read_json(\"history.json\")\n",
    "        # Add user response to history\n",
    "        history.loc[history.index[-1], \"user_response\"] = user_response\n",
    "\n",
    "        # Check user response for questions that are specified to check\n",
    "        if (next_question_document is not None) and (next_question_document.metadata['check'] == 1):\n",
    "            # Check if a follow up question is needed based on user response and the question asked\n",
    "            assessment = evaluate_response(user_response, question_asked)\n",
    "            needFollowUp = True if assessment[\"Assessment\"] == \"Yes\" else False\n",
    "            # If need follow up question, ask the question again\n",
    "            if needFollowUp:\n",
    "                # Allow only one follow-up per question\n",
    "                if clarified:\n",
    "                    clarified = False\n",
    "                    pass\n",
    "                else:\n",
    "                    clarified = True\n",
    "                    # TO DO: Improve the instruction or construct a LLM chain to ask the question again.\n",
    "                    follow_up_question = generateFollowUp(user_response, question_asked)\n",
    "                    print('\\n')\n",
    "                    print(f\"LLM: {follow_up_question}\")\n",
    "                    # Wait for user input\n",
    "                    user_response = input()\n",
    "                    print('\\n')\n",
    "                    print(\"User: \", end='')\n",
    "                    print(user_response)\n",
    "\n",
    "                    # Saving the question that the RAG chain has chosen to history\n",
    "                    new_row = pd.DataFrame({'id': [next_question_id], 'question': [question_asked], 'llm_question': [follow_up_question], 'user_response': [\"\"], 'stage': [next_question_document.metadata['stage']]})\n",
    "                    history = pd.concat([history, new_row], ignore_index=True)\n",
    "                    history.to_json(\"history.json\", orient=\"records\")\n",
    "                    continue\n",
    "\n",
    "        \n",
    "        # Survey flow\n",
    "        if len(db.docstore._dict) == 0 and stage is None:\n",
    "            stage = 0\n",
    "            db = stage_0_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 0:\n",
    "            stage = 1\n",
    "            db = stage_1_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 1:\n",
    "            stage = 2\n",
    "            db = stage_2_db\n",
    "        if len(db.docstore._dict) == 0 and stage == 2:\n",
    "            stage = 3\n",
    "            db = stage_3_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 3:\n",
    "            history.to_json(\"history.json\", orient=\"records\")\n",
    "            # To end the survey gracefully\n",
    "            end = end_survey(user_response, question_asked)\n",
    "            print('\\n')\n",
    "            print(f\"LLM: {end}\")\n",
    "            break\n",
    "\n",
    "        ## Ask the next best question based on previous survey user response\n",
    "        # Create new retriever object with updated vectorstore\n",
    "        retriever = get_retriever(db)\n",
    "        # Create new RAG chain with updated retriever\n",
    "        qa_chain = get_rag_chain(retriever)\n",
    "        print('\\n')\n",
    "        print(\"LLM: \", end='')\n",
    "        # Get LLM reply, next question to ask and its question id\n",
    "        llm_reply, next_question_document, next_question_id = get_llm_outputs(qa_chain, user_response, question_asked)\n",
    "        # Get question asked\n",
    "        question_asked = get_question_asked(next_question_document)\n",
    "        # Updated vectorstore with asked question removed\n",
    "        db = remove_question_from_db(db, next_question_document)\n",
    "\n",
    "        # Saving the question that the RAG chain has chosen to history\n",
    "        new_row = pd.DataFrame({'id': [next_question_id], 'question': [next_question_document.page_content], 'llm_question': [llm_reply], 'user_response': [\"\"], \"stage\": [next_question_document.metadata['stage']]})\n",
    "        history = pd.concat([history, new_row], ignore_index=True)\n",
    "        history.to_json(\"history.json\", orient=\"records\")\n",
    "\n",
    "    # Wait for user input\n",
    "    user_response = input()\n",
    "    #user_response = f'The user\\'s response is:{user_response}'\n",
    "    print('\\n')\n",
    "    print(\"User: \", end='')\n",
    "    print(user_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My To Dos:\n",
    "# Explore feature: adding conversation history into the RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mysql.connector\n",
    "\n",
    "load_dotenv()\n",
    "mysql_root_password = os.getenv(\"MYSQL_ROOT_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../history.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/elisalin/DSA3101-Project/backend testing/survey_backend.ipynb Cell 58\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elisalin/DSA3101-Project/backend%20testing/survey_backend.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elisalin/DSA3101-Project/backend%20testing/survey_backend.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#run api test to get history.json\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/elisalin/DSA3101-Project/backend%20testing/survey_backend.ipynb#Y111sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m../../history.json\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/elisalin/DSA3101-Project/backend%20testing/survey_backend.ipynb#Y111sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     hist \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../history.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#run api test to get history.json\n",
    "with open('../../history.json', 'r') as file:\n",
    "    hist = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the n-th user_response in chat log\n",
    "def get_r(hist, id):\n",
    "    value = []\n",
    "    for chat in hist:\n",
    "        if chat['id'] == id:\n",
    "            value.append(chat['user_response'])\n",
    "    return ','.join(value)\n",
    "\n",
    "#accepts chat log and updates database\n",
    "def update_db(history):\n",
    "    #connect to database\n",
    "    db = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        port=3307,\n",
    "        user=\"root\",\n",
    "        password=mysql_root_password,\n",
    "    )\n",
    "    mycursor = db.cursor()\n",
    "\n",
    "    #add to database\n",
    "    mycursor.execute(\"USE testdatabase\")\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Stage_0(hair_length, hair_type, hair_concerns, scalp_type, scalp_concerns, hair_treatment) VALUES (%s,%s,%s,%s,%s,%s)\", (get_r(history,4), get_r(history,5), get_r(history,6), get_r(history,7), get_r(history,8), get_r(history,9))\n",
    "    )\n",
    "    stage0_id = mycursor.lastrowid\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Stage_1(wash_frequency, hair_products, styling_products, prod_switch_freq, salon_freq, hair_goal, hair_health_importance) VALUES (%s,%s,%s,%s,%s,%s,%s)\", (get_r(history,10), get_r(history,11), get_r(history,12), get_r(history,13), get_r(history,14), get_r(history,15),get_r(history,16))\n",
    "    )\n",
    "    stage1_id = mycursor.lastrowid\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Stage_2(pantene_prod, pantene_info, most_fav_product, least_fav_product, prod_effectiveness, prod_recommend, desired_ingredients) VALUES (%s,%s,%s,%s,%s,%s,%s)\", (get_r(history,17), get_r(history,18), get_r(history,19), get_r(history,20), get_r(history,21), get_r(history,22),get_r(history,23))\n",
    "    )\n",
    "    stage2_id = mycursor.lastrowid\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Stage_3(important_factors, preferred_price_range, purchase_method) VALUES (%s,%s,%s)\", (get_r(history,24), get_r(history,25), get_r(history,26))\n",
    "    )\n",
    "    stage3_id = mycursor.lastrowid\n",
    "\n",
    "    mycursor.execute(\n",
    "        \"INSERT INTO Demographic(name, age, gender, stage0_id, stage1_id, stage2_id, stage3_id) VALUES (%s,%s,%s,%s,%s,%s,%s)\", (get_r(history,1), get_r(history,2), get_r(history,3),stage0_id,stage1_id,stage2_id,stage3_id)\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "\n",
    "    mycursor.close()\n",
    "    db.close()\n",
    "    print('db updated')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test one response per question\n",
    "test_json = [{\"id\":1,\"question\":\"What is your name?\",\"user_response\":\"the user's response is 'ti'\"},{\"id\":3,\"question\":\"what is your gender identity?\",\"user_response\":\"the user's response is 'male'\"},{\"id\":2,\"question\":\"What is your age group?\",\"user_response\":\"the user's response is '799'\"},{\"id\":4,\"question\":\"What is your hair length?\",\"user_response\":\"the user's response is 'lengthy'\"},{\"id\":5,\"question\":\"What is your hair type?\",\"user_response\":\"the user's response is 'brownian motion'\"},{\"id\":7,\"question\":\"What is your scalp type?\",\"user_response\":\"the user's response is 'the second law of thermodynamics'\"},{\"id\":6,\"question\":\"What are your hair concerns?\",\"user_response\":\"the user's response is 'e do be equal to mc squared'\"},{\"id\":8,\"question\":\"What are your scalp concerns?\",\"user_response\":\"the user's response is 'eahahahahhaha'\"},{\"id\":9,\"question\":\"What hair treatments have you done?\",\"user_response\":\"the user's response is 'no hair treatments'\"},{\"id\":11,\"question\":\"What hair products do you use regularly?\",\"user_response\":\"the user's response is \\\"none\\\"\"},{\"id\":15,\"question\":\"What is your ideal hair goal?\",\"user_response\":\"\"},{\"id\":16,\"user_response\":\"awwd\"},{\"id\":12,\"user_response\":\"a\"},{\"id\":13,\"user_response\":\"a\"},{\"id\":14,\"user_response\":\"a\"},{\"id\":10,\"user_response\":\"a\"},{\"id\":17,\"user_response\":\"a\"},{\"id\":18,\"user_response\":\"a\"},{\"id\":19,\"user_response\":\"a\"},{\"id\":20,\"user_response\":\"a\"},{\"id\":21,\"user_response\":\"a\"},{\"id\":22,\"user_response\":\"a\"},{\"id\":23,\"user_response\":\"a\"},{\"id\":24,\"user_response\":\"a\"},{\"id\":25,\"user_response\":\"a\"},{\"id\":26,\"user_response\":\"a\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test multiple response for same question\n",
    "test_double = [{\"id\":23,\"question\":\"What hair product improvements would you like to see in the future?\",\"llm_question\":\"I'm glad to hear that you're taking the time to share your thoughts with us! It seems like you're not aware of any Pantene product series at the moment. That's totally okay! We're always looking to improve and innovate, so I'm curious: what hair product improvements would you like to see in the future? Your input is truly valuable to us.\",\"user_response\":\"None\",\"stage\":2},\n",
    "    {\"id\":23,\"question\":\"What hair product improvements would you like to see in the future?\",\"llm_question\":\" I'm sorry for any confusion, but it seems like I didn't receive a response from you yet regarding the hair product improvements you'd like to see in the future. Your insights are valuable to us, so could you please share what changes or enhancements you'd like to see in hair products?\",\"user_response\":\"Ok priec\",\"stage\":2}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to docker before running (docker start test-mysql)\n",
    "update_db(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
