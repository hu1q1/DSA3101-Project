{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LLM Capabilities for Convey - An Interactive Survey Interface\n",
    "\n",
    "In this notebook, we will explore the capabilities of Large Language Models (LLMs) for our project in building an interactive survey interface. We'll focus on the following tasks:\n",
    "\n",
    "## 1. RAG (Retrieval-Augmented Generation)\n",
    "- Implementing and fine-tuning RAG for tasks such as responding and asking follow-up questions to users in a personalised manner.\n",
    "- Exploring RAG's ability to provide relevant product-specific responses based on retrieval from a knowledge source.\n",
    "\n",
    "## 2. Prompt Engineering\n",
    "- Crafting effective prompts to guide the LLM's responses.\n",
    "- Experimenting with different prompt formats and strategies to optimise performance.\n",
    "\n",
    "## 3. Vector Store Manipulation\n",
    "- Manipulating vector stores to enhance the understanding and generation capabilities of the LLM.\n",
    "- Examining the impact of vector store modifications on the quality and relevance of generated responses.\n",
    "\n",
    "We'll use this notebook to test various features and functionalities provided by the LLM and assess its suitability for the Convey platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "1. Create and activate a virtual environment before running the command below to install the necessary Python packages.\n",
    "2. Create a hugging face api token and store it in the current working directory in a .env file as follows:\n",
    "\n",
    "    HUGGINGFACEHUB_API_TOKEN=\"hf_***************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import pipeline\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Hugging Face Hub API Token into OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from local .env file if available\n",
    "if os.path.isfile('.env'):\n",
    "    # Set path to api key\n",
    "    dotenv_path = Path('.env')\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "else:\n",
    "    load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Using Survey Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Survey Questions and Creating Document Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_questions = [\n",
    "    #{'id': 1, 'question': \"What is your name?\", \"check_user_response\": 0},   # This question is taken out and assumed as the first survey question\n",
    "    {'id': 2, 'question': \"What is your age group?\", \"check_user_response\": 0},\n",
    "    {'id': 3, 'question': \"what is your gender identity?\", \"check_user_response\": 0},\n",
    "]\n",
    "\n",
    "# Creating Document objects for survey questions\n",
    "demographic_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"category\":\"demographics\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in demographic_questions\n",
    "]\n",
    "\n",
    "stage_0_questions = [\n",
    "    {'id': 4, 'question': \"What is your hair length?\", \"check_user_response\": 0},\n",
    "    {'id': 5, 'question': \"What is your hair type?\", \"check_user_response\": 0},\n",
    "    {'id': 6, 'question': \"What are your hair concerns?\", \"check_user_response\": 0},\n",
    "    {'id': 7, 'question': \"What is your scalp type?\", \"check_user_response\": 0},\n",
    "    {'id': 8, 'question': \"What are your scalp concerns?\", \"check_user_response\": 0},\n",
    "    {'id': 9, 'question': \"What hair treatments have you done?\", \"check_user_response\": 0},\n",
    "]\n",
    "# Creating Document objects for survey questions\n",
    "stage_0_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": \"0\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_0_questions\n",
    "]\n",
    "\n",
    "stage_1_questions = [\n",
    "    {'id': 10, 'question': \"How often do you wash your hair?\", \"check_user_response\": 0},\n",
    "    {'id': 11, 'question': \"What hair products do you use regularly?\", \"check_user_response\": 0},\n",
    "    {'id': 12, 'question': \"What hair styling products do you use regularly?\", \"check_user_response\": 0},\n",
    "    {'id': 13, 'question': \"How often do you switch hair product brands?\", \"check_user_response\": 0},\n",
    "    {'id': 14, 'question': \"How often do you visit hair salons or barber shops?\", \"check_user_response\": 0},\n",
    "    {'id': 15, 'question': \"What is your ideal hair goal?\", \"check_user_response\": 0},\n",
    "    {'id': 16, 'question': \"How important is hair health to you?\", \"check_user_response\": 0},\n",
    "]\n",
    "stage_1_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": \"1\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_1_questions\n",
    "]\n",
    "\n",
    "stage_2_questions = [\n",
    "    {'id': 17, 'question': \"Which of the following Pantene product series (collections) are you aware of?\", \"check_user_response\": 0},\n",
    "    {'id': 18, 'question': \"From where did you know pantene?\", \"check_user_response\": 0},\n",
    "    {'id': 19, 'question': \"What is your favorite pantene product and what do you like about it?\", \"check_user_response\": 0},\n",
    "    {'id': 20, 'question': \"what is your least favorite pantene product and what do you dislike about it?\", \"check_user_response\": 0},\n",
    "    {'id': 21, 'question': \"How would you rate the effectiveness of your favorite / least favorite pantene product?\", \"check_user_response\": 0},\n",
    "    {'id': 22, 'question': \"Would you recommend your current hair products to others? Why?\", \"check_user_response\": 1},\n",
    "    {'id': 23, 'question': \"What hair product improvements would you like to see in the future?\", \"check_user_response\": 1},\n",
    "]\n",
    "stage_2_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": \"2\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_2_questions\n",
    "]\n",
    "\n",
    "stage_3_questions = [\n",
    "    {'id': 24, 'question': \"When choosing hair products, how important are the following factors to you?\", \"check_user_response\": 0},\n",
    "    {'id': 25, 'question': \"What is your preferred price range for hair products?\", \"check_user_response\": 0},\n",
    "    {'id': 26, 'question': \"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\", \"check_user_response\": 1},\n",
    "]\n",
    "stage_3_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": \"3\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_3_questions\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Embedding Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an embedding model from Hugging Face\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='all-MiniLM-L6-v2', \n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employing FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorstore for the documents/survey questions\n",
    "demographic_db = FAISS.from_documents(\n",
    "    demographic_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "# Saving the vectorstore in local directory - persistence\n",
    "demographic_db.save_local(\"demographic_questions\")\n",
    "# Loading the vectorstore from local directory\n",
    "demographic_db = FAISS.load_local(\"demographic_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_0_db = FAISS.from_documents(\n",
    "    stage_0_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_0_db.save_local(\"stage_0_questions\")\n",
    "stage_0_db = FAISS.load_local(\"stage_0_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_1_db = FAISS.from_documents(\n",
    "    stage_1_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_1_db.save_local(\"stage_1_questions\")\n",
    "stage_1_db = FAISS.load_local(\"stage_1_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_2_db = FAISS.from_documents(\n",
    "    stage_2_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_2_db.save_local(\"stage_2_questions\")\n",
    "stage_2_db = FAISS.load_local(\"stage_2_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_3_db = FAISS.from_documents(\n",
    "    stage_3_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_3_db.save_local(\"stage_3_questions\")\n",
    "stage_3_db = FAISS.load_local(\"stage_3_questions\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"30 years old\"\n",
    "\n",
    "demographic_db.similarity_search_with_score(text, k=1, filter=dict(category='demographics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Open-source LLM from Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# ENDPOINT_URL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=256,\n",
    "    #top_k=50,\n",
    "    temperature=0.01,\n",
    "    #repetition_penalty=1.03,\n",
    "    return_full_text=False,\n",
    "    # callbacks=callbacks,\n",
    "    streaming=True,\n",
    "    stop_sequences=['</s>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever with Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(vectorstore: FAISS):\n",
    "    # Setting retriever to only retrieve the best follow-up question \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "    return retriever\n",
    "\n",
    "retriever = get_retriever(demographic_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating First Survey Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the first question\n",
    "first_question = llm.invoke(\"[INST]I am doing a survey. Greet me excitedly and ask me what is my age. Do not add anything.[/INST]\") #in a fairy tale setting\n",
    "\n",
    "first_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chat Log Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging of chat\n",
    "def create_chat_log():\n",
    "    memory = ConversationBufferMemory(return_messages=False, memory_key='chat_history')\n",
    "    return memory\n",
    "\n",
    "def add_to_chat_log(chat_log, message_type: str, message: str):\n",
    "    if message_type == 'ai':\n",
    "        chat_log.chat_memory.add_ai_message(message)\n",
    "    else:\n",
    "        chat_log.chat_memory.add_user_message(message)\n",
    "\n",
    "def get_chat_history(chat_log):\n",
    "    chat_history = chat_log.load_memory_variables({})['chat_history']\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "chat_log = create_chat_log()\n",
    "add_to_chat_log(chat_log, message_type='ai', message=first_question)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.runnables import RunnableLambda - to be used for multiple arguments input\n",
    "\n",
    "def get_rag_chain(retriever):\n",
    "    # General prompt for all questions\n",
    "    prompt_template = \"\"\"You are a friendly survey interface assistant.\n",
    "        You are given a survey question, a survey user response to that question, the sentiment of the user response and a follow-up question below.\n",
    "        Reply to the survey user response kindly and just ask the follow-up question. Do not say anything else.\n",
    "        Do not ask any other questions.\n",
    "\n",
    "        Question: {previous_question}\n",
    "        User response: {user_response}\n",
    "        User sentiment: {sentiment}\n",
    "        Follow-up question: {next_question}\n",
    "        \n",
    "        Reply:\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=['previous_question', 'user_response', 'next_question', 'sentiment']\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        # return \"\\n\\n\".join(doc.metadata['prompt'] + '\\n' + doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        # Retrieve next best question\n",
    "        RunnableParallel({\"docs\": itemgetter(\"user_response\") | retriever, \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        # Optional: Format question to ask user\n",
    "        | ({\"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"next_question\": lambda x: format_docs(x['docs']), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        # Optional: Prompt Engineering - Each question to have their own prompt template for LLM to ask the question\n",
    "        | ({\"docs\": lambda x: x['docs'], \"prompt\": prompt, \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"next_question\": itemgetter(\"next_question\"), \"previous_question\": itemgetter(\"previous_question\")}) \n",
    "        # Output results\n",
    "        | ({\"answer\": itemgetter(\"prompt\") | llm | StrOutputParser(), \"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "    )\n",
    "    return rag_chain \n",
    "\n",
    "\n",
    "rag_chain = get_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking RAG Chain with User Response to First Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_response = \"I am Xiao Ming.\"\n",
    "add_to_chat_log(chat_log, message_type='user', message=user_response)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of user response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error() \n",
    "\n",
    "def get_user_sentiment(user_response: str):\n",
    "    pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    user_sentiment = pipe(user_response)[0]['label']\n",
    "    return user_sentiment\n",
    "\n",
    "user_sentiment = get_user_sentiment(user_response)\n",
    "user_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_rag_chain(rag_chain, user_response: str, user_sentiment: str, previous_question: str):\n",
    "    output = {}\n",
    "    for chunk in rag_chain.stream(dict(user_response=user_response, sentiment=user_sentiment, previous_question=previous_question)):\n",
    "        for key in chunk:\n",
    "            if key not in output:\n",
    "                output[key] = chunk[key].strip() if key == 'answer' else chunk[key]\n",
    "            # if key == 'answer':\n",
    "                # new_token = chunk[key]\n",
    "                # yield new_token\n",
    "                # output[key] += new_token\n",
    "            else:\n",
    "                output[key] += chunk[key]\n",
    "            if key == 'answer':\n",
    "                print(chunk[key], end=\"\", flush=True)\n",
    "    return output\n",
    "    \n",
    "def get_llm_outputs(rag_chain, user_response: str, previous_question: str):\n",
    "    user_sentiment = get_user_sentiment(user_response)\n",
    "    output = invoke_rag_chain(rag_chain, user_response, user_sentiment, previous_question)\n",
    "    # LLM reply to output to frontend\n",
    "    llm_reply = output['answer']\n",
    "    # Get document of question asked by LLM \n",
    "    next_question_document = output['docs'][0]\n",
    "    # id of question asked to output to frontend \n",
    "    next_question_id = next_question_document.metadata['id']\n",
    "    return llm_reply, next_question_document, next_question_id\n",
    "\n",
    "\n",
    "llm_reply, next_question_document, next_question_id = get_llm_outputs(rag_chain, user_response, first_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Asked Question from Vector Store Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_question_from_db(vectorstore: FAISS, document_to_delete: Document):\n",
    "    count = 0\n",
    "    for key, item in vectorstore.docstore._dict.items():\n",
    "        count += 1\n",
    "        if item == document_to_delete:\n",
    "            break\n",
    "    if count >= 0:\n",
    "        vectorstore.delete([vectorstore.index_to_docstore_id[count-1]])\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "print(len(demographic_db.docstore._dict))\n",
    "demographic_db = remove_question_from_db(demographic_db, next_question_document)\n",
    "print(len(demographic_db.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"coherence\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"ahhahahah\"\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input='What is your gender identity?',\n",
    "    )\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_user_response(question,response):\n",
    "    eval_result = evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input=question,\n",
    "    )\n",
    "\n",
    "    return eval_result['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Simulation\n",
    "\n",
    "Make sure to run the above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Vector Store From Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_db = FAISS.load_local(\"demographic_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_0_db = FAISS.load_local(\"stage_0_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_1_db = FAISS.load_local(\"stage_1_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_2_db = FAISS.load_local(\"stage_2_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_3_db = FAISS.load_local(\"stage_3_questions\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain to end the survey:\n",
    "def end_survey():\n",
    "    print('\\n')\n",
    "    print(\"It was interesting to get to know more about you! Thank you for participating in the survey!\")\n",
    "    print(\"If you have any further questions or feedback, feel free to reach out to us.\")\n",
    "\n",
    "# Get question asked\n",
    "def get_question_asked(question_document):\n",
    "    # Retrieve original question based on question_id\n",
    "    return question_document.page_content\n",
    "\n",
    "# get_question_asked(next_question_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_log = create_chat_log()\n",
    "stage = None # Change this for testing different stages\n",
    "db = demographic_db\n",
    "retriever = get_retriever(demographic_db)\n",
    "question_asked = \"What is your name?\"\n",
    "user_response = \"\"\n",
    "next_question_document = None\n",
    "clarified = False\n",
    "\n",
    "first_question = llm.invoke(f\"[INST]I am starting to answer a survey. Greet me and ask me: {question_asked}[/INST]\")\n",
    "print(f\"LLM: {first_question}\")\n",
    "\n",
    "while True:\n",
    "    # User responded\n",
    "    if user_response:\n",
    "        # Check user response for questions that are specified to check\n",
    "        if (next_question_document is not None) and (next_question_document.metadata['check'] == 1):\n",
    "            # Check if user response is coherent with the question asked\n",
    "            isCoherent = True if verify_user_response(question_asked, user_response) == 'Y' else False\n",
    "            # If not coherent, ask the question again\n",
    "            if not isCoherent:\n",
    "                # Allow only one clarification per question i.e. repeat the question once\n",
    "                if clarified:\n",
    "                    clarified = False\n",
    "                    pass\n",
    "                else:\n",
    "                    clarified = True\n",
    "                    # TO DO: Improve the instruction or construct a LLM chain to ask the question again.\n",
    "                    repeat_question = llm.invoke(f\"[INST] As the survey user did not answer the question correctly. Ask the question kindly again: {question_asked} [/INST]\")\n",
    "                    print('\\n')\n",
    "                    print(f\"LLM: {repeat_question}\")\n",
    "                    # Wait for user input\n",
    "                    user_response = input()\n",
    "                    print('\\n')\n",
    "                    print(\"User: \", end='')\n",
    "                    print(user_response)\n",
    "                    add_to_chat_log(chat_log, message_type='user', message=user_response)\n",
    "                    continue\n",
    "        \n",
    "        # Survey flow\n",
    "        if len(db.docstore._dict) == 0 and stage is None:\n",
    "            stage = 0\n",
    "            db = stage_0_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 0:\n",
    "            stage = 1\n",
    "            db = stage_1_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 1:\n",
    "            stage = 2\n",
    "            db = stage_2_db\n",
    "        if len(db.docstore._dict) == 0 and stage == 2:\n",
    "            stage = 3\n",
    "            db = stage_3_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 3:\n",
    "            # To end the survey gracefully\n",
    "            end_survey()\n",
    "            break\n",
    "\n",
    "        ## Ask the next best question based on previous survey user response\n",
    "        # Create new retriever object with updated vectorstore\n",
    "        retriever = get_retriever(db)\n",
    "        # Create new RAG chain with updated retriever\n",
    "        qa_chain = get_rag_chain(retriever)\n",
    "        print('\\n')\n",
    "        print(\"LLM: \", end='')\n",
    "        # Get LLM reply, next question to ask and its question id\n",
    "        llm_reply, next_question_document, next_question_id = get_llm_outputs(qa_chain, user_response, question_asked)\n",
    "        # Get question asked\n",
    "        question_asked = get_question_asked(next_question_document)\n",
    "        add_to_chat_log(chat_log, message_type='ai', message=llm_reply)\n",
    "        # Updated vectorstore with asked question removed\n",
    "        db = remove_question_from_db(db, next_question_document)\n",
    "        \n",
    "        \n",
    "    # Wait for user input\n",
    "    user_response = input()\n",
    "    print('\\n')\n",
    "    print(\"User: \", end='')\n",
    "    print(user_response)\n",
    "    add_to_chat_log(chat_log, message_type='user', message=user_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
