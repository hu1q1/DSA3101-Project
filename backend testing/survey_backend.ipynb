{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LLM Capabilities for Convey - An Interactive Survey Interface\n",
    "\n",
    "In this notebook, we will explore the capabilities of Large Language Models (LLMs) for our project in building an interactive survey interface. We'll focus on the following tasks:\n",
    "\n",
    "## 1. RAG (Retrieval-Augmented Generation)\n",
    "- Implementing and fine-tuning RAG for tasks such as responding and asking follow-up questions to users in a personalised manner.\n",
    "- Exploring RAG's ability to provide relevant product-specific responses based on retrieval from a knowledge source.\n",
    "\n",
    "## 2. Prompt Engineering\n",
    "- Crafting effective prompts to guide the LLM's responses.\n",
    "- Experimenting with different prompt formats and strategies to optimise performance.\n",
    "\n",
    "## 3. Vector Store Manipulation\n",
    "- Manipulating vector stores to enhance the understanding and generation capabilities of the LLM.\n",
    "- Examining the impact of vector store modifications on the quality and relevance of generated responses.\n",
    "\n",
    "We'll use this notebook to test various features and functionalities provided by the LLM and assess its suitability for the Convey platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "1. Create and activate a virtual environment before running the command below to install the necessary Python packages.\n",
    "2. Create a hugging face api token and store it in the current working directory in a .env file as follows:\n",
    "\n",
    "    HUGGINGFACEHUB_API_TOKEN=\"hf_***************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import pipeline\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Hugging Face Hub API Token into OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from local .env file if available\n",
    "if os.path.isfile('.env'):\n",
    "    # Set path to api key\n",
    "    dotenv_path = Path('.env')\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "else:\n",
    "    load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store Using Survey Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Survey Questions and Creating Document Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_questions = [\n",
    "    #{'id': 1, 'question': \"What is your name?\", \"check_user_response\": 0},   # This question is taken out and assumed as the first survey question\n",
    "    {'id': 2, 'question': \"What is your age group?\", \"check_user_response\": 0},\n",
    "    {'id': 3, 'question': \"what is your gender identity?\", \"check_user_response\": 0},\n",
    "]\n",
    "\n",
    "# Creating Document objects for survey questions\n",
    "demographic_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"category\":\"demographics\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in demographic_questions\n",
    "]\n",
    "\n",
    "stage_0_questions = [\n",
    "    {'id': 4, 'question': \"What is your hair length?\", \"check_user_response\": 0},\n",
    "    {'id': 5, 'question': \"What is your hair type?\", \"check_user_response\": 0},\n",
    "    {'id': 6, 'question': \"What are your hair concerns?\", \"check_user_response\": 0},\n",
    "    {'id': 7, 'question': \"What is your scalp type?\", \"check_user_response\": 0},\n",
    "    {'id': 8, 'question': \"What are your scalp concerns?\", \"check_user_response\": 0},\n",
    "    {'id': 9, 'question': \"What hair treatments have you done?\", \"check_user_response\": 0},\n",
    "]\n",
    "# Creating Document objects for survey questions\n",
    "stage_0_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": \"0\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_0_questions\n",
    "]\n",
    "\n",
    "stage_1_questions = [\n",
    "    {'id': 10, 'question': \"How often do you wash your hair?\", \"check_user_response\": 0},\n",
    "    {'id': 11, 'question': \"What hair products do you use regularly?\", \"check_user_response\": 0},\n",
    "    {'id': 12, 'question': \"What hair styling products do you use regularly?\", \"check_user_response\": 0},\n",
    "    {'id': 13, 'question': \"How often do you switch hair product brands?\", \"check_user_response\": 0},\n",
    "    {'id': 14, 'question': \"How often do you visit hair salons or barber shops?\", \"check_user_response\": 0},\n",
    "    {'id': 15, 'question': \"What is your ideal hair goal?\", \"check_user_response\": 0},\n",
    "    {'id': 16, 'question': \"How important is hair health to you?\", \"check_user_response\": 0},\n",
    "]\n",
    "stage_1_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": \"1\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_1_questions\n",
    "]\n",
    "\n",
    "stage_2_questions = [\n",
    "    {'id': 17, 'question': \"Which of the following Pantene product series (collections) are you aware of?\", \"check_user_response\": 0},\n",
    "    {'id': 18, 'question': \"From where did you know Pantene?\", \"check_user_response\": 0},\n",
    "    {'id': 19, 'question': \"What is your favorite Pantene product and what do you like about it?\", \"check_user_response\": 0},\n",
    "    {'id': 20, 'question': \"What is your least favorite Pantene product and what do you dislike about it?\", \"check_user_response\": 0},\n",
    "    {'id': 21, 'question': \"How would you rate the overall effectiveness Pantene products?\", \"check_user_response\": 0},\n",
    "    {'id': 22, 'question': \"Would you recommend your current hair products to others? Why?\", \"check_user_response\": 1},\n",
    "    {'id': 23, 'question': \"What hair product improvements would you like to see in the future?\", \"check_user_response\": 1},\n",
    "]\n",
    "stage_2_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": \"2\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_2_questions\n",
    "]\n",
    "\n",
    "stage_3_questions = [\n",
    "    {'id': 24, 'question': \"When choosing hair products, how important are the following factors to you?\", \"check_user_response\": 0},\n",
    "    {'id': 25, 'question': \"What is your preferred price range for hair products?\", \"check_user_response\": 0},\n",
    "    {'id': 26, 'question': \"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\", \"check_user_response\": 1},\n",
    "]\n",
    "stage_3_documents = [\n",
    "    Document(\n",
    "        page_content=question['question'],\n",
    "        metadata={\n",
    "            \"id\": question['id'],\n",
    "            \"stage\": \"3\",\n",
    "            \"check\": question['check_user_response']\n",
    "        }\n",
    "    ) for question in stage_3_questions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt templates\n",
    "demographic_prompt_template = \"Hey there! Welcome to the survey! We're thrilled to have you on board. Let's kick things off by getting to know you a little better. Please take a moment to answer the following demographic questions:\\n{}\"\n",
    "stage_0_prompt_template = \"Great! Now, let's talk about your hair care routine. We're here to make sure our products match your needs perfectly. Share your thoughts with us:\\n{}\"\n",
    "stage_1_prompt_template = \"Awesome! We're diving deeper into your hair care habits and preferences. Your feedback is invaluable in helping us improve. Let's get started:\\n{}\"\n",
    "stage_2_prompt_template = \"You're doing great! Now, we're eager to hear what you think about Pantene products. Your insights will shape our future offerings. Share your thoughts with us:\\n{}\"\n",
    "stage_3_prompt_template = \"Almost there! We're curious about your shopping preferences and priorities. Let's wrap up with a few more questions:\\n{}\"\n",
    "\n",
    "# Few-shot examples\n",
    "# Demographic Questions\n",
    "demographic_few_shot_examples = [\n",
    "    (\"What is your age group?\", \"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"Above 65\"),\n",
    "    (\"What is your gender identity?\", \"Male\", \"Female\", \"Non-binary\", \"Prefer not to share\")\n",
    "]\n",
    "\n",
    "# Stage 0 Questions\n",
    "stage_0_few_shot_examples = [\n",
    "    (\"What is your hair length?\", \"Short\", \"Medium\", \"Long\", \"No hair\"),\n",
    "    (\"What is your hair type?\", \"Curly\", \"straight\", \"wavy\", \"dry\", \"normal\", \"oily\", \"thin\", \"thick\"),\n",
    "    (\"What are your hair concerns?\", \"Frizzy\", \"dry\", \"split ends\", \"hair loss\", \"breakage\", \"none\", \"others\"),\n",
    "    (\"What is your scalp type?\", \"Oily\", \"dry\", \"normal\"),\n",
    "    (\"What are your scalp concerns?\", \"Itchiness\", \"sensitive\", \"allergies\", \"dandruff\", \"dryness\", \"none\", \"others\"),\n",
    "    (\"What hair treatments have you done?\", \"Keratin treatments\", \"dyed\", \"permed\", \"bleached\", \"none\", \"others\")\n",
    "]\n",
    "\n",
    "# Stage 1 Questions\n",
    "stage_1_few_shot_examples = [\n",
    "    (\"How often do you wash your hair?\", \"Daily\", \"several times a day\", \"every other day\", \"others\"),\n",
    "    (\"What hair products do you use regularly?\", \"shampoo\", \"conditioner\", \"leave-in treatments\", \"hair masks\"),\n",
    "    (\"What hair styling products do you use regularly?\", \"gel\", \"hair dryer\", \"flat iron\", \"curler\", \"mousses\", \"serums\", \"others\"),\n",
    "    (\"How often do you switch hair product brands?\", \"every few months\", \"every year\", \"every few years\", \"I do not switch\"),\n",
    "    (\"How often do you visit hair salons or barber shops?\", \"every few weeks\", \"every few months\", \"once a year\", \"I do not visit\"),\n",
    "    (\"What is your ideal hair goal?\", \"Shiny\", \"healthy\", \"volume\", \"smoothness\", \"others\"),\n",
    "    (\"How important is hair health to you?\", \"Very important\", \"1\", \"5\", \"7\", \"10\")\n",
    "]\n",
    "\n",
    "# Stage 2 Questions\n",
    "stage_2_few_shot_examples = [\n",
    "    (\"Which of the following Pantene product series (collections) are you aware of?\", \"Pantene Pro-V\", \"Hair Care Shampoo and Conditioner\", \"I don't know any\"),\n",
    "    (\"From where did you know Pantene?\", \"TV commercials\", \"word of mouth\", \"retail shops\", \"social media\", \"others\"),\n",
    "    (\"What is your favorite Pantene product and what do you like about it?\", \"Pro-V shampoo, makes my hair soft\", \"conditioner, smells nice\"),\n",
    "    (\"What is your least favorite Pantene product and what do you dislike about it?\", \"Pantene conditioner, weighs down my hair\", \"conditioner, makes my hair fall\"),\n",
    "    (\"How would you rate the overall effectiveness Pantene products?\", \"Highly effective\", \"1\", \"5\", \"7\", \"10\"),\n",
    "    (\"Would you recommend your current hair products to others? Why?\", \"Yes, they make my hair feel great\", \"yes, they are affordabe\", \"no, there are better brands\", \"no, they made me drop more hair\"),\n",
    "    (\"What hair product improvements would you like to see in the future?\", \"More natural ingredients\", \"cheaper\", \"more benefits in a product\")\n",
    "]\n",
    "\n",
    "# Stage 3 Questions\n",
    "stage_3_few_shot_examples = [\n",
    "    (\"When choosing hair products, how important are the following factors to you?\", \"natural or synthetic ingredients\", \"fragrance\", \"specific certifications\", \"specific claims\", \"price\", \"celebrity endorsements or influencer recommendations\", \"specific hair concerns\", \"long-lasting effects\", \"multi-functional benefits\", \"eco-friendly or sustainable packaging\", \"hair stylists for salon professionals\", \"packaging\", \"advertising campaigns or promotions\"), \n",
    "    (\"What is your preferred price range for hair products?\", \"under $10\", \"$10-50\", \"$50-100\", \"above $100\"),\n",
    "    (\"Do you prefer to purchase hair products online or in-store? If in-store, which stores?\", \"Online, Amazon\", \"online, shopee\", \"in store, NTUC\", \"in-store, salons\")\n",
    "]\n",
    "\n",
    "# Connect prompt templates for smooth conversation flow\n",
    "demographic_prompt = demographic_prompt_template.format(\"\\n\".join([q['question'] for q in demographic_questions]))\n",
    "stage_0_prompt = stage_0_prompt_template.format(\"\\n\".join([q['question'] for q in stage_0_questions]))\n",
    "stage_1_prompt = stage_1_prompt_template.format(\"\\n\".join([q['question'] for q in stage_1_questions]))\n",
    "stage_2_prompt = stage_2_prompt_template.format(\"\\n\".join([q['question'] for q in stage_2_questions]))\n",
    "stage_3_prompt = stage_3_prompt_template.format(\"\\n\".join([q['question'] for q in stage_3_questions]))\n",
    "\n",
    "# Define function to select prompt based on the stage of the survey\n",
    "def get_prompt(stage):\n",
    "    if stage == 0:\n",
    "        return stage_0_prompt\n",
    "    elif stage == 1:\n",
    "        return stage_1_prompt\n",
    "    elif stage == 2:\n",
    "        return stage_2_prompt\n",
    "    elif stage == 3:\n",
    "        return stage_3_prompt\n",
    "    else:\n",
    "        return \"Invalid stage number\"\n",
    "\n",
    "# Example usage:\n",
    "current_stage = 0\n",
    "current_prompt = get_prompt(current_stage)\n",
    "print(current_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Embedding Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an embedding model from Hugging Face\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='all-MiniLM-L6-v2', \n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employing FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorstore for the documents/survey questions\n",
    "demographic_db = FAISS.from_documents(\n",
    "    demographic_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "# Saving the vectorstore in local directory - persistence\n",
    "demographic_db.save_local(\"demographic_questions\")\n",
    "# Loading the vectorstore from local directory\n",
    "demographic_db = FAISS.load_local(\"demographic_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_0_db = FAISS.from_documents(\n",
    "    stage_0_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_0_db.save_local(\"stage_0_questions\")\n",
    "stage_0_db = FAISS.load_local(\"stage_0_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_1_db = FAISS.from_documents(\n",
    "    stage_1_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_1_db.save_local(\"stage_1_questions\")\n",
    "stage_1_db = FAISS.load_local(\"stage_1_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_2_db = FAISS.from_documents(\n",
    "    stage_2_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_2_db.save_local(\"stage_2_questions\")\n",
    "stage_2_db = FAISS.load_local(\"stage_2_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "stage_3_db = FAISS.from_documents(\n",
    "    stage_3_documents,\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "stage_3_db.save_local(\"stage_3_questions\")\n",
    "stage_3_db = FAISS.load_local(\"stage_3_questions\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='What is your age group?', metadata={'id': 2, 'category': 'demographics', 'check': 0}),\n",
       "  0.946681)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"30 years old\"\n",
    "\n",
    "demographic_db.similarity_search_with_score(text, k=1, filter=dict(category='demographics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising an Open-source LLM from Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\user\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "ENDPOINT_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# ENDPOINT_URL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=256,\n",
    "    #top_k=50,\n",
    "    temperature=0.01,\n",
    "    #repetition_penalty=1.03,\n",
    "    return_full_text=False,\n",
    "    # callbacks=callbacks,\n",
    "    streaming=True,\n",
    "    stop_sequences=['</s>'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Retriever with Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(vectorstore: FAISS):\n",
    "    # Setting retriever to only retrieve the best follow-up question \n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "    return retriever\n",
    "\n",
    "retriever = get_retriever(demographic_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating First Survey Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello there! I'm so excited to be part of your survey. I was wondering if you could tell me your age? Thank you!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask the first question\n",
    "first_question = llm.invoke(\"[INST]I am doing a survey. Greet me excitedly and ask me what is my age. Do not add anything.[/INST]\") #in a fairy tale setting\n",
    "\n",
    "first_question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Chat Log Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI:  Hello there! I'm so excited to be part of your survey. I was wondering if you could tell me your age? Thank you!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logging of chat\n",
    "def create_chat_log():\n",
    "    memory = ConversationBufferMemory(return_messages=False, memory_key='chat_history')\n",
    "    return memory\n",
    "\n",
    "def add_to_chat_log(chat_log, message_type: str, message: str):\n",
    "    if message_type == 'ai':\n",
    "        chat_log.chat_memory.add_ai_message(message)\n",
    "    else:\n",
    "        chat_log.chat_memory.add_user_message(message)\n",
    "\n",
    "def get_chat_history(chat_log):\n",
    "    chat_history = chat_log.load_memory_variables({})['chat_history']\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "chat_log = create_chat_log()\n",
    "add_to_chat_log(chat_log, message_type='ai', message=first_question)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.runnables import RunnableLambda - to be used for multiple arguments input\n",
    "\n",
    "def get_rag_chain(retriever):\n",
    "    # General prompt for all questions\n",
    "    prompt_template = \"\"\"You are a friendly survey interface assistant.\n",
    "        You are given a survey question, a survey user response to that question, the sentiment of the user response and a follow-up question below.\n",
    "        Reply to the survey user response kindly and just ask the follow-up question.\n",
    "\n",
    "        Question: {previous_question}\n",
    "        User response: {user_response}\n",
    "        User sentiment: {sentiment}\n",
    "        Follow-up question: {next_question}\n",
    "        \n",
    "        Reply:\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=['previous_question', 'user_response', 'next_question', 'sentiment']\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        # return \"\\n\\n\".join(doc.metadata['prompt'] + '\\n' + doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        # Retrieve next best question\n",
    "        RunnableParallel({\"docs\": itemgetter(\"user_response\") | retriever, \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        # Optional: Format question to ask user\n",
    "        | ({\"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"next_question\": lambda x: format_docs(x['docs']), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "        # Optional: Prompt Engineering - Each question to have their own prompt template for LLM to ask the question\n",
    "        | ({\"docs\": lambda x: x['docs'], \"prompt\": prompt, \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"next_question\": itemgetter(\"next_question\"), \"previous_question\": itemgetter(\"previous_question\")}) \n",
    "        # Output results\n",
    "        | ({\"answer\": itemgetter(\"prompt\") | llm | StrOutputParser(), \"docs\": lambda x: x['docs'], \"user_response\": itemgetter(\"user_response\"), \"sentiment\": itemgetter(\"sentiment\"), \"previous_question\": itemgetter(\"previous_question\")})\n",
    "    )\n",
    "    return rag_chain \n",
    "\n",
    "\n",
    "rag_chain = get_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking RAG Chain with User Response to First Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: The user's response is:he\\nAI: What is your scalp type?\\nHuman: The user's response is:bald\\nAI: What are your hair concerns?\\nHuman: The user's response is:noneeee\\nAI: What is your hair length?\\nHuman: The user's response is:hadwhwioda\\nAI: What are your scalp concerns?\\nHuman: The user's response is:dawjdlaiwuf\\nAI: What hair treatments have you done?\\nHuman: The user's response is:none\\nAI: What is your ideal hair goal?\\nHuman: The user's response is:\\nAI: How important is hair health to you?\\nHuman: The user's response is:\\nAI: That's interesting! What hair products do you use regularly?\\nHuman: I am Xiao Ming.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_response = \"I am Xiao Ming.\"\n",
    "add_to_chat_log(chat_log, message_type='user', message=user_response)\n",
    "get_chat_history(chat_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of user response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.set_verbosity_error() \n",
    "\n",
    "def get_user_sentiment(user_response: str):\n",
    "    pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "    user_sentiment = pipe(user_response)[0]['label']\n",
    "    return user_sentiment\n",
    "\n",
    "user_sentiment = get_user_sentiment(user_response)\n",
    "user_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello Xiao Ming! That's a great name. Now, let me ask you, how often do you wash your hair? Thank you for your response!"
     ]
    }
   ],
   "source": [
    "def invoke_rag_chain(rag_chain, user_response: str, user_sentiment: str, previous_question: str):\n",
    "    output = {}\n",
    "    for chunk in rag_chain.stream(dict(user_response=user_response, sentiment=user_sentiment, previous_question=previous_question)):\n",
    "        for key in chunk:\n",
    "            if key not in output:\n",
    "                output[key] = chunk[key].strip() if key == 'answer' else chunk[key]\n",
    "            # if key == 'answer':\n",
    "                # new_token = chunk[key]\n",
    "                # yield new_token\n",
    "                # output[key] += new_token\n",
    "            else:\n",
    "                output[key] += chunk[key]\n",
    "            if key == 'answer':\n",
    "                print(chunk[key], end=\"\", flush=True)\n",
    "    return output\n",
    "    \n",
    "def get_llm_outputs(rag_chain, user_response: str, previous_question: str):\n",
    "    user_sentiment = get_user_sentiment(user_response)\n",
    "    output = invoke_rag_chain(rag_chain, user_response, user_sentiment, previous_question)\n",
    "    # LLM reply to output to frontend\n",
    "    llm_reply = output['answer']\n",
    "    # Get document of question asked by LLM \n",
    "    next_question_document = output['docs'][0]\n",
    "    # id of question asked to output to frontend \n",
    "    next_question_id = next_question_document.metadata['id']\n",
    "    return llm_reply, next_question_document, next_question_id\n",
    "\n",
    "\n",
    "llm_reply, next_question_document, next_question_id = get_llm_outputs(rag_chain, user_response, first_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Asked Question from Vector Store Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorstore\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(demographic_db\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39m_dict))\n\u001b[1;32m---> 13\u001b[0m demographic_db \u001b[38;5;241m=\u001b[39m \u001b[43mremove_question_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdemographic_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_question_document\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(demographic_db\u001b[38;5;241m.\u001b[39mdocstore\u001b[38;5;241m.\u001b[39m_dict))\n",
      "Cell \u001b[1;32mIn[44], line 8\u001b[0m, in \u001b[0;36mremove_question_from_db\u001b[1;34m(vectorstore, document_to_delete)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m     vectorstore\u001b[38;5;241m.\u001b[39mdelete([\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_to_docstore_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vectorstore\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "def remove_question_from_db(vectorstore: FAISS, document_to_delete: Document):\n",
    "    count = 0\n",
    "    for key, item in vectorstore.docstore._dict.items():\n",
    "        count += 1\n",
    "        if item == document_to_delete:\n",
    "            break\n",
    "    if count >= 0:\n",
    "        vectorstore.delete([vectorstore.index_to_docstore_id[count-1]])\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "print(len(demographic_db.docstore._dict))\n",
    "demographic_db = remove_question_from_db(demographic_db, next_question_document)\n",
    "print(len(demographic_db.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"coherence\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Step 1: Determine if the submission is coherent.\\n\\nStep 2: The submission is \"ahhahahah\" which does not make any sense in the context of the question asked.\\n\\nStep 3: Therefore, the submission is not coherent.\\n\\nN',\n",
       " 'value': 'N',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = \"ahhahahah\"\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input='What is your gender identity?',\n",
    "    )\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_user_response(question,response):\n",
    "    eval_result = evaluator.evaluate_strings(\n",
    "        prediction=response,\n",
    "        input=question,\n",
    "    )\n",
    "\n",
    "    return eval_result['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Simulation\n",
    "\n",
    "Make sure to run the above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Vector Store From Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_db = FAISS.load_local(\"demographic_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_0_db = FAISS.load_local(\"stage_0_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_1_db = FAISS.load_local(\"stage_1_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_2_db = FAISS.load_local(\"stage_2_questions\", embedding_model, allow_dangerous_deserialization=True)\n",
    "stage_3_db = FAISS.load_local(\"stage_3_questions\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain to end the survey:\n",
    "def end_survey():\n",
    "    print('\\n')\n",
    "    print(\"It was interesting to get to know more about you! Thank you for participating in the survey!\")\n",
    "    print(\"If you have any further questions or feedback, feel free to reach out to us.\")\n",
    "\n",
    "# Get question asked\n",
    "def get_question_asked(question_document):\n",
    "    # Retrieve original question based on question_id\n",
    "    return question_document.page_content\n",
    "\n",
    "# get_question_asked(next_question_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM:  Hello! I'm here to assist you with your survey. To get started, could you please tell me what your name is? I'll do my best to help you answer the survey questions to the best of your ability. Thank you!\n",
      "\n",
      "\n",
      "User: The user's response is:no\n",
      "\n",
      "\n",
      "LLM:  I see, thank you for sharing that. Now, may I know what is your gender identity?\n",
      "\n",
      "User: The user's response is:male\n",
      "\n",
      "\n",
      "LLM:  Thank you for sharing that. May I know what your age group is?\n",
      "\n",
      "User: The user's response is:banana\n",
      "\n",
      "\n",
      "LLM:  I see, you've entered banana as your age group. That's quite unique! Moving on, could you please tell me what your hair type is?\n",
      "\n",
      "User: The user's response is:black\n",
      "\n",
      "\n",
      "LLM:  Thank you for your response. By any chance, do you know what your scalp type is?\n",
      "\n",
      "User: The user's response is:bald\n",
      "\n",
      "\n",
      "LLM:  I'm sorry to hear that. What are your hair concerns?\n",
      "\n",
      "User: The user's response is:none\n",
      "\n",
      "\n",
      "LLM:  That's great to hear that you don't have any hair concerns! To help us recommend the best products for you, could you please tell us what is your hair length?\n",
      "\n",
      "User: The user's response is:5cm exact\n",
      "\n",
      "\n",
      "LLM:  That's great! And what are your scalp concerns?\n",
      "\n",
      "User: The user's response is:you shut your mouth\n",
      "\n",
      "\n",
      "LLM:  Sure, I understand. What hair treatments have you done?\n",
      "\n",
      "User: The user's response is:none\n",
      "\n",
      "\n",
      "LLM:  That's great! I see you haven't done any hair treatments yet. So, what is your ideal hair goal?\n",
      "\n",
      "User: The user's response is:none\n",
      "\n",
      "\n",
      "LLM:  I see, no specific hair goal. That's alright. How important is hair health to you?\n",
      "\n",
      "User: The user's response is:very\n",
      "\n",
      "\n",
      "LLM:  That's great to hear that hair health is very important to you! How often do you visit hair salons or barber shops?\n",
      "\n",
      "User: The user's response is:everyday\n",
      "\n",
      "\n",
      "LLM:  That's interesting! How often do you wash your hair?\n",
      "\n",
      "User: The user's response is:never\n",
      "\n",
      "\n",
      "LLM:  That's interesting! How often do you switch hair product brands?\n",
      "\n",
      "User: The user's response is:all the time\n",
      "\n",
      "\n",
      "LLM:  That's interesting! Could you tell me what hair products you use regularly?\n",
      "\n",
      "User: The user's response is:you\n",
      "\n",
      "\n",
      "LLM:  That's great! Could you also tell me what hair styling products you use regularly?\n",
      "\n",
      "User: The user's response is:none\n",
      "\n",
      "\n",
      "LLM:  Thank you for your response! I see that you don't use any hair styling products regularly. Moving on to the next question, what is your least favorite Pantene product and what do you dislike about it?\n",
      "\n",
      "User: The user's response is:ypu\n",
      "\n",
      "\n",
      "LLM:  Thank you for your response! I'm curious, what hair product improvements would you like to see in the future?\n",
      "\n",
      "User: The user's response is:all of them\n",
      "\n",
      "\n",
      "LLM:  I'm here to help with any questions you may have. It seems that the previous response to the question about hair product improvements didn't provide the information needed. To rephrase the question, could you please share what changes or improvements you would like to see in hair care products in the future? Your input can help improve the development of future hair care solutions.\n",
      "\n",
      "\n",
      "User: none\n",
      "\n",
      "\n",
      "LLM:  Thank you for your response! That's helpful to know. By the way, which of the following Pantene product series (collections) are you aware of?\n",
      "\n",
      "User: The user's response is:those\n",
      "\n",
      "\n",
      "LLM:  Thank you for your response! Could you please share how you would rate the effectiveness of your favorite / least favorite Pantene product?\n",
      "\n",
      "User: The user's response is:alkhfaehfa\n",
      "\n",
      "\n",
      "LLM:  Thank you for your responses was neutral. That's interesting. From where did you know pantene?\n",
      "\n",
      "User: The user's response is:you lol\n",
      "\n",
      "\n",
      "LLM:  That's great! I see you have a sense of humor. Now, could you tell me what is your favorite pantene product and what do you like about it?\n",
      "\n",
      "User: The user's response is:milkshake\n",
      "\n",
      "\n",
      "LLM:  That's great to hear that you like the Milkshake product! Would you recommend your current hair products to others? Why?\n",
      "\n",
      "User: The user's response is:no i dont wanna :(\n",
      "\n",
      "\n",
      "LLM:  I'm here to help you with your survey. It seems like the previous response to the question was not provided in the correct format. I'll ask the question again so we can get a valid response.\n",
      "\n",
      "Kindly share your thoughts on the hair products you are currently using: Would you recommend them to others? If yes, could you please explain why you would suggest these products to others? Your feedback is valuable to us and will help improve our understanding of customer preferences. Thank you!\n",
      "\n",
      "\n",
      "User: no\n",
      "\n",
      "\n",
      "LLM:  I see, thank you for sharing your thoughts. By the way, what is your preferred price range for hair products?\n",
      "\n",
      "User: The user's response is:free. free. free. free.\n",
      "\n",
      "\n",
      "LLM:  Thank you for your response! Do you prefer to purchase hair products online or in-store? If in-store, which stores?\n",
      "\n",
      "User: The user's response is:in storeeeeee\n",
      "\n",
      "\n",
      "LLM:  Thank you for your response! When choosing hair products, how important are the following factors to you?\n",
      "\n",
      "User: The user's response is:none\n",
      "\n",
      "\n",
      "It was interesting to get to know more about you! Thank you for participating in the survey!\n",
      "If you have any further questions or feedback, feel free to reach out to us.\n"
     ]
    }
   ],
   "source": [
    "chat_log = create_chat_log()\n",
    "stage = None # Change this for testing different stages\n",
    "db = demographic_db\n",
    "retriever = get_retriever(demographic_db)\n",
    "question_asked = \"What is your name?\"\n",
    "user_response = \"\"\n",
    "next_question_document = None\n",
    "clarified = False\n",
    "\n",
    "first_question = llm.invoke(f\"[INST]I am starting to answer a survey. Greet me and ask me: {question_asked}[/INST]\")\n",
    "print(f\"LLM: {first_question}\")\n",
    "\n",
    "while True:\n",
    "    # User responded\n",
    "    if user_response:\n",
    "        # Check user response for questions that are specified to check\n",
    "        if (next_question_document is not None) and (next_question_document.metadata['check'] == 1):\n",
    "            # Check if user response is coherent with the question asked\n",
    "            isCoherent = True if verify_user_response(question_asked, user_response) == 'Y' else False\n",
    "            # If not coherent, ask the question again\n",
    "            if not isCoherent:\n",
    "                # Allow only one clarification per question i.e. repeat the question once\n",
    "                if clarified:\n",
    "                    clarified = False\n",
    "                    pass\n",
    "                else:\n",
    "                    clarified = True\n",
    "                    # TO DO: Improve the instruction or construct a LLM chain to ask the question again.\n",
    "                    repeat_question = llm.invoke(f\"[INST] As the survey user did not answer the question correctly. Ask the question kindly again: {question_asked} [/INST]\")\n",
    "                    print('\\n')\n",
    "                    print(f\"LLM: {repeat_question}\")\n",
    "                    # Wait for user input\n",
    "                    user_response = input()\n",
    "                    print('\\n')\n",
    "                    print(\"User: \", end='')\n",
    "                    print(user_response)\n",
    "                    add_to_chat_log(chat_log, message_type='user', message=user_response)\n",
    "                    continue\n",
    "        \n",
    "        # Survey flow\n",
    "        if len(db.docstore._dict) == 0 and stage is None:\n",
    "            stage = 0\n",
    "            db = stage_0_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 0:\n",
    "            stage = 1\n",
    "            db = stage_1_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 1:\n",
    "            stage = 2\n",
    "            db = stage_2_db\n",
    "        if len(db.docstore._dict) == 0 and stage == 2:\n",
    "            stage = 3\n",
    "            db = stage_3_db\n",
    "        elif len(db.docstore._dict) == 0 and stage == 3:\n",
    "            # To end the survey gracefully\n",
    "            end_survey()\n",
    "            break\n",
    "\n",
    "        ## Ask the next best question based on previous survey user response\n",
    "        # Create new retriever object with updated vectorstore\n",
    "        retriever = get_retriever(db)\n",
    "        # Create new RAG chain with updated retriever\n",
    "        qa_chain = get_rag_chain(retriever)\n",
    "        print('\\n')\n",
    "        print(\"LLM: \", end='')\n",
    "        # Get LLM reply, next question to ask and its question id\n",
    "        llm_reply, next_question_document, next_question_id = get_llm_outputs(qa_chain, user_response, question_asked)\n",
    "        # Get question asked\n",
    "        question_asked = get_question_asked(next_question_document)\n",
    "        add_to_chat_log(chat_log, message_type='ai', message=llm_reply)\n",
    "        # Updated vectorstore with asked question removed\n",
    "        db = remove_question_from_db(db, next_question_document)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Wait for user input\n",
    "    user_response = input()\n",
    "    user_response = f'The user\\'s response is:{user_response}'\n",
    "    print('\\n')\n",
    "    print(\"User: \", end='')\n",
    "    print(user_response)\n",
    "    add_to_chat_log(chat_log, message_type='user', message=user_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mysql.connector\n",
    "\n",
    "load_dotenv()\n",
    "mysql_root_password = os.getenv(\"MYSQL_ROOT_PASSWORD\")\n",
    "\n",
    "#connect to database\n",
    "db = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    port=3307,\n",
    "    user=\"root\",\n",
    "    password=mysql_root_password,\n",
    ")\n",
    "\n",
    "mycursor = db.cursor()\n",
    "\n",
    "#get values from history.json\n",
    "def update_db(history):\n",
    "    \n",
    "\n",
    "#get values\n",
    "\n",
    "\n",
    "#insert values\n",
    "#mycursor.execute(\"INSERT INTO Survey(columns) VALUES (%s,%s,%s)\", (values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
